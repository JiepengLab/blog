# 模拟卷

1. 事件发生的概率越小，自信息越大。(T)

2. 两个相互独立的随机变量的联合自信息等于两个变量的自信息之和。(T)

3. 连续信源和离散信源的熵都有非负性。(F)

4. 当随机变量 \(X\) 和 \(Y\) 相互独立时，条件熵 \(H(X|Y)\) 等于信源熵 \(H(Y)\)。(F)

5. 各码字的长度符合克拉夫特不等式，是唯一可译码存在的充分和必要条件。(T)

6. 不存在码长为 \(\{1, 2, 2, 3\}\) 的唯一可译码。(T)

7. 对于任何二元贝努利信源，全零序列都不是一个典型列。(F)

8. 当随机变量 \(X\) 和 \(Y\) 相互独立时，\(I(X; Y) = H(X)\)。(F)

9. 对一个离散信源进行二元 Huffman 编码，则发生概率最低的两个符号其码长必然相等。(F)

10. 一般情况下，Huffman 编码的效率大于香农编码。(T)
