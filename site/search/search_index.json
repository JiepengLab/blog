{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to JiepengLab","text":"<p>The lab is for my personal research and study. I will share my research and study notes here.</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/","title":"Mathematics for Deep Learning","text":""},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#broadcasting","title":"Broadcasting","text":"\\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 1 &amp; 2 &amp; 3 \\end{bmatrix} \\\\ \\end{aligned}\\] \\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 2 &amp; 2 \\end{bmatrix} \\\\ \\end{aligned}\\]"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#hadamard-product","title":"Hadamard product","text":"\\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\odot \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; 4 &amp; 9 \\\\ 16 &amp; 25 &amp; 36 \\end{bmatrix} \\\\ \\end{aligned}\\]"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#vector-and-matrix-norms","title":"Vector and Matrix Norms","text":"<p>The \\(L_p\\) norm of a vector \\(x \\in \\mathbb{R}^n\\) is defined as</p> \\[ \\|x\\|_p = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\] <p>The \\(L_2\\) norm is also called the Euclidean norm.</p> <p>\\(\\|x\\|_1 = \\sum_{i=1}^n |x_i|\\) \\(\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\\) \\(\\|x\\|_\\infty = \\max_{i=1,\\ldots,n} |x_i|\\)</p> <p>Frobenius norm of a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) is defined as</p> \\[ \\|A\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}\\]"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#eigen-and-singular-value-decomposition","title":"Eigen and Singular Value Decomposition","text":""},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#eigenvalue-decompositionevd","title":"Eigenvalue Decomposition(EVD)","text":"<p>Only be applied to square matrices</p> \\[A = V \\Lambda V^{-1}\\] <p>\\(V\\) is a matrix whose columns are the eigenvectors of \\(A\\). \\(\\Lambda\\) is a diagonal matrix whose diagonal entries are the eigenvalues of \\(A\\).</p> <p>Each real symmetric matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) can be decomposed into</p> \\[A = Q \\Lambda Q^T\\] <p>where \\(Q\\) is an orthogonal matrix whose columns are the eigenvectors of \\(A\\) and \\(\\Lambda\\) is a diagonal matrix whose diagonal entries are the eigenvalues of \\(A\\).</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#singular-value-decompositionsvd","title":"Singular Value Decomposition(SVD)","text":"<p>For non-square matrices we can use singular value decomposition</p> \\[A = U D V^T\\] <p>\\(U\\) and \\(V\\) are orthogonal matrices. \\(D\\) is a diagonal matrix whose diagonal entries are the singular values of \\(A\\). \\(U\\) and \\(V\\) are the left and right singular vectors of \\(A\\).</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#relationship-between-evd-and-svd","title":"Relationship between EVD and SVD","text":"\\[A = U D V^T\\] \\[A^T A = V D^T U^T U D V^T = V D^T D V^T= V D^2 V^T\\] \\[A A^T = U D V^T V D U^T = U D^T D U^T = U D^2 U^T\\] <p>The right singular vectors \\(V\\) of \\(A\\) are the eigenvectors of \\(A^T A\\). The left singular vectors \\(U\\) of \\(A\\) are the eigenvectors of \\(A A^T\\). The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^T A\\) or \\(A A^T\\).</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#application-principal-component-analysis-pca","title":"Application: Principal Component Analysis (PCA)","text":"<p>PCA is a technique for analyzing large high-dimensional datasets</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#probability","title":"Probability","text":"<p>To do</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#information-and-entropy","title":"Information and Entropy","text":"<p>To satisfy all requirements, we define the self-information of an event \\(X = x\\) as</p> \\[ I(x) = -\\log P(x) \\quad \\text{[unit:nat]}\\] <p>Self-information is also interpreted as quantifying the level of \u201csurprise\u201d When using base-2 logarithms, units are called \u201cbits\u201d or \u201cshannons\u201d</p>"},{"location":"CS/Note_00_Mathematics%20for%20Deep%20Learning/#the-argmin-and-argmax-operators","title":"The Argmin and Argmax Operators","text":"<p>Let \\(\\mathcal{X}\\) be a set and \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\) be a function. The argmin of \\(f\\) is defined as</p> \\[ \\arg \\min_{x \\in \\mathcal{X}} f(x) = \\{x \\in \\mathcal{X} | f(x) = \\min_{x' \\in \\mathcal{X}} f(x')\\}\\] <p>The argmax of \\(f\\) is defined as</p> \\[ \\arg \\max_{x \\in \\mathcal{X}} f(x) = \\{x \\in \\mathcal{X} | f(x) = \\max_{x' \\in \\mathcal{X}} f(x')\\}\\]"},{"location":"CS/Note_01_introduction/","title":"1 Computer Vision \u2013 Summer 2022","text":""},{"location":"CS/Note_01_introduction/#11-organization","title":"1.1 Organization","text":"<p>Goal: Students gain an understanding of the theoretical and practical concepts of computer vision using deep neural networks and graphical models. A strong focus is put on 3D vision. After this course, students should be able to develop and train computer vision models, reproduce research results and conduct original research.</p> <p>Course website: https://uni-tuebingen.de/de/203146</p> <p>Exercises are offered every 2 weeks, 6 assignments in total</p>"},{"location":"CS/Note_01_introduction/#12-course-materials","title":"1.2 Course Materials","text":"<p>Books:</p> <ul> <li>Szeliski: Computer Vision: Algorithms and Applications</li> <li>Hartley and Zisserman: Multiple View Geometry in Computer Vision</li> <li>Nowozin and Lampert: Structured Learning and Prediction in Computer Vision</li> <li>Goodfellow, Bengio, Courville: Deep Learning</li> <li>Deisenroth, Faisal, Ong: Mathematics for Machine Learning</li> <li>Inoffical lecture notes written by students in summer 2021</li> </ul> <p>Tutorials:</p> <ul> <li>The Python Tutorial</li> <li>NumPy Quickstart</li> <li>PyTorch Tutorial</li> </ul> <p>Frameworks / IDEs:</p> <ul> <li>Visual Studio Code</li> <li>Google Colab</li> </ul> <p>Related Courses:</p> <ul> <li>Gkioulekas (CMU): Computer Vision</li> <li>Owens (University of Michigan): Foundations of Computer Vision</li> <li>Lazebnik (UIUC): Computer Vision</li> <li>Freeman and Isola (MIT): Advances in Computer Vision</li> <li>Seitz (University of Washington): Computer Vision</li> <li>Slide Decks covering Szeliski Book</li> </ul>"},{"location":"CS/Note_01_introduction/#13-prerequisites","title":"1.3 Prerequisites","text":"<p>Math:</p> <ul> <li>Linear algebra, probability and information theory. If unsure, have a look at: Goodfellow et al.: Deep Learning (Book), Chapters 1-4</li> <li>Luxburg: Mathematics for Machine Learning (Lecture)</li> <li>Deisenroth et al.:Mathematics for Machine Learning (Book)</li> </ul> <p>Computer Science:</p> <ul> <li>Variables, functions, loops, classes, algorithms</li> </ul> <p>Deep Learning:</p> <ul> <li>Geiger: Deep Learning (Lecture)</li> </ul> <p>Python and PyTorch:</p> <ul> <li>The Python Tutorial</li> <li>PyTorch Tutorial</li> </ul>"},{"location":"CS/Note_01_introduction/#14-introduction","title":"1.4 Introduction","text":"<p>Goal of Computer Vision is to convert light into meaning (geometric, semantic, . . . )</p>"},{"location":"CS/Note_01_introduction/#15-history","title":"1.5 History","text":"<p>Hard to say...</p>"},{"location":"CS/Note_02_Image_Formation/","title":"2 Image_Formation","text":""},{"location":"CS/Note_02_Image_Formation/#21-primitives-and-transformations","title":"2.1 Primitives and Transformations","text":"<p>Geometric primitives(\u51e0\u4f55\u56fe\u5143) are the basic building blocks used to describe 3D shapes, such as points, lines, planes, and polygons.</p>"},{"location":"CS/Note_02_Image_Formation/#211-2d-primitives","title":"2.1.1 2D Primitives","text":""},{"location":"CS/Note_02_Image_Formation/#2d-points","title":"2D Points","text":"<p>2D points can be written in inhomogeneous coordinates(\u975e\u9f50\u6b21\u5750\u6807) as</p> \\[ \\mathbf{x} =(x,y)^T\\in \\mathbb{R}^2\\] <p>or in homogeneous coordinates(\u9f50\u6b21\u5750\u6807) as</p> \\[ \\widetilde{\\mathbf{x}} =\\left(\\widetilde{x},\\widetilde{y},\\widetilde{w}\\right)^T\\in \\mathbb{P}^2\\] <p>where \\(\\mathbb{P}^2 ( = \\mathbb{R}^3)\\) is the projective plane(\u5c04\u5f71\u7a7a\u95f4) and \\(\\widetilde{\\mathbf{x}}\\) is a projective point(\u6295\u5f71\u70b9).</p> <p>The tilde sign (~) is a convention for homogeneous coordinates. Homogeneous vectors are defined only up to scale.</p> <p>Inhomogeneous vector \\(\\mathbf{x}\\) is converted to a homogeneous vector \\(\\widetilde{\\mathbf{x}}\\) as follows:</p> \\[ \\widetilde{\\mathbf{x} }=\\begin{pmatrix}\\widetilde{x}\\\\\\widetilde{y}\\\\\\widetilde{w}\\end{pmatrix}=\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix}=\\begin{pmatrix}\\mathbf{x}\\\\1\\end{pmatrix}= \\overline{\\mathbf{x} }\\] <p>where \\(\\overline{x}\\) is the augmented vector(\u589e\u5e7f\u5411\u91cf) of \\(\\mathbf{x}\\).</p> <p>We say augmented vector \\(\\overline{\\mathbf{x} }\\) for all homogeneous vectors which last coordinate is equal to 1. Special vector: Homogeneous points whose last element is \\(\\widetilde{w}=0\\) are called ideal points or points at infinity. These points can\u2019t be represented with inhomogeneous coordinates!</p>"},{"location":"CS/Note_02_Image_Formation/#2d-lines","title":"2D Lines","text":"<p>2D lines can also be expressed using homogeneous coordinates(\u9f50\u6b21\u5750\u6807) \\(\\mathbf{\\widetilde{l}}=(a,b,c)^T\\) : $$ {\\mathbf{\\overline{x}}|{\\mathbf{\\overline{x}}}^T\\mathbf{\\widetilde{l}}=0} \\Leftrightarrow {x,y | ax+by+c=0}$$</p> <p>\u5de6\u5230\u53f3\u662f\u56e0\u4e3a \\(\\mathbf{\\overline{x}}^T\\mathbf{\\widetilde{l}}=0 \\Rightarrow (x,y,1)\\begin{pmatrix}a\\\\b\\\\c\\end{pmatrix}=0 \\Rightarrow ax+by+c=0\\)</p> <p>\u6807\u51c6\u5316 \\(\\mathbf{\\widetilde{l}}\\)\uff1a\\(\\mathbf{\\widetilde{l}}=(n_x,n_y,d)^T\\)\uff0c \u5176\u4e2d \\(n_x^2+n_y^2=1\\)\uff0c \\((n_x,n_y)\\) \u662f \\(\\mathbf{\\widetilde{l}}\\) \u7684\u6cd5\u5411\u91cf \uff0c\\(d\\) \u662f \\(\\mathbf{\\widetilde{l}}\\) \u5230\u539f\u70b9\u7684\u8ddd\u79bb.</p> <p>An exception is the line at infinity \\(\\mathbf{\\widetilde{l}}_{\\infty}=(0,0,1)^T\\), which passes through all ideal points.</p>"},{"location":"CS/Note_02_Image_Formation/#2d-line-arithmetic","title":"2D Line Arithmetic","text":"<p>In homogeneous coordinates, the intersection of two lines is given by: $$ \\mathbf{\\overline{x}}=\\mathbf{\\widetilde{l}}_1\\times \\mathbf{\\widetilde{l}}_2$$</p> <p>where \\(\\times\\) is the cross product of two vectors.</p> <p>Similarly, the intersection of two points is given by: $$ \\mathbf{\\widetilde{l}}=\\mathbf{\\overline{x}}_1\\times \\mathbf{\\overline{x}}_2$$</p>"},{"location":"CS/Note_02_Image_Formation/#2d-conics","title":"2D Conics(\u4e8c\u6b21\u66f2\u7ebf)","text":"<p>More complex algebraic objects can be represented using polynomial homogeneous equations. For example, conic sections (arising as the intersection of a plane and a 3D cone) can be written using quadric equations:</p> \\[ \\mathbf{\\overline{x}}^T\\mathbf{Q}\\mathbf{\\overline{x}}=0\\] <p></p>"},{"location":"CS/Note_02_Image_Formation/#212-3d-primitives","title":"2.1.2 3D Primitives","text":""},{"location":"CS/Note_02_Image_Formation/#3d-points","title":"3D Points","text":"<p>3D points can be written in inhomogeneous coordinates as $$ \\mathbf{x}=(x,y,z)^T\\in \\mathbb{R}^3$$ or in homogeneous coordinates as $$ \\widetilde{\\mathbf{x}}=(\\widetilde{x},\\widetilde{y},\\widetilde{z},\\widetilde{w})^T\\in \\mathbb{P}^3$$</p> <p>where \\(\\mathbb{P}^3 (=\\mathbb{R}^4)\\)  is the projective space.</p>"},{"location":"CS/Note_02_Image_Formation/#3d-planes","title":"3D Planes","text":"<p>3D planes can be expressed using homogeneous coordinates \\(\\mathbf{\\widetilde{m}}=(a,b,c,d)^T\\): $$ {\\mathbf{\\overline{x}}|{\\mathbf{\\overline{x}}}^T\\mathbf{\\widetilde{m}}=0} \\Leftrightarrow {x,y,z | ax+by+cz+d=0}$$</p> <p>\u6807\u51c6\u5316 \\(\\mathbf{\\widetilde{m}}\\)\uff1a\\(\\mathbf{\\widetilde{m}}=(n_x,n_y,n_z,d)^T\\) \u5176\u4e2d \\(n_x^2+n_y^2+n_z^2=1\\), \\((n_x,n_y,n_z)\\) \u662f \\(\\mathbf{\\widetilde{m}}\\) \u7684\u6cd5\u5411\u91cf\uff0c\\(d\\) \u662f \\(\\mathbf{\\widetilde{m}}\\) \u5230\u539f\u70b9\u7684\u8ddd\u79bb\u3002</p> <p>An exception is the plane at infinity \\(\\mathbf{\\widetilde{m}}_{\\infty}=(0,0,0,1)^T\\), which passes through all ideal points.</p>"},{"location":"CS/Note_02_Image_Formation/#3d-lines","title":"3D Lines","text":"<p>3D lines are less elegant than either 2D lines or 3D planes. One possible representation is to express points on a line as a linear combination of two points \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) on the line:</p> \\[ \\{\\mathbf{x}|\\mathbf{x}=(1-\\lambda)\\mathbf{p}+\\lambda\\mathbf{q},\\lambda\\in \\mathbb{R}\\}\\] <p>However, this representation uses 6 parameters(\\(\\mathbf{p}\\)\u548c\\(\\mathbf{q}\\)\u5206\u522b\u4e09\u4e2a\u53c2\u6570) for 4 degrees of freedom(\u4e00\u4e2a\u70b9(3\u4e2a\u4e0d\u81ea\u7531\u5ea6)+\u65b9\u5411(1\u4e2a\u4e0d\u81ea\u7531\u5ea6)).</p> <p>\u6240\u4ee5\u53ef\u4ee5\u7528two-plane parameterization\uff08\u4e24\u5e73\u9762\u53c2\u6570\u5316\uff09\u6765\u8868\u793a3D line.</p>"},{"location":"CS/Note_02_Image_Formation/#3d-quadrics","title":"3D Quadrics(\u4e8c\u6b21\u66f2\u9762)","text":"<p>The 3D analog of 2D conics is a quadric surface:</p> \\[ \\mathbf{\\overline{x}}^T\\mathbf{Q}\\mathbf{\\overline{x}}=0\\] <p></p> <p>Superquadrics: generalization of quadrics</p>"},{"location":"CS/Note_02_Image_Formation/#213-2d-transformations","title":"2.1.3 2D Transformations","text":""},{"location":"CS/Note_02_Image_Formation/#translation","title":"Translation","text":"<p>Translation: (2D Translation of the Input, 2 DoF(\u4e24\u4e2a\u81ea\u7531\u5ea6: \\(t_x\\)\u548c\\(t_y\\)))</p> \\[ \\mathbf{x}'=\\mathbf{x}+\\mathbf{t} \\Leftrightarrow \\begin{pmatrix}x'\\\\y'\\\\1\\end{pmatrix}=\\begin{bmatrix}1&amp;0&amp;t_x\\\\0&amp;1&amp;t_y\\\\0&amp;0&amp;1\\end{bmatrix}\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix} \\Leftrightarrow \\mathbf{\\overline{x}}'=\\begin{bmatrix}\\mathbf{I}&amp;\\mathbf{t}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\mathbf{\\overline{x}}\\] <ul> <li>Using homogeneous representations allows to chain/invert transformations.</li> <li>Augmented vectors \\(\\mathbf{\\overline{x}}\\) can always be replaced by general homogeneous ones \\(\\widetilde{\\mathbf{x}}\\).</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#euclidean","title":"Euclidean","text":"<p>Euclidean: (2D Translation + 2D Rotation, 3 DoF)</p> \\[ \\mathbf{x}'=\\mathbf{R}\\mathbf{x}+\\mathbf{t} \\Leftrightarrow \\begin{pmatrix}x'\\\\y'\\\\1\\end{pmatrix}=\\begin{bmatrix}\\cos\\theta&amp;-\\sin\\theta&amp;t_x\\\\\\sin\\theta&amp;\\cos\\theta&amp;t_y\\\\0&amp;0&amp;1\\end{bmatrix}\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix} \\Leftrightarrow \\mathbf{\\overline{x}}'=\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\mathbf{\\overline{x}}\\] <ul> <li>\\(\\mathbf{R} \\in SO(2)\\): special orthogonal group of 2D rotations,which is an orthogonal rotation matrix with \\(\\mathbf{R}^T\\mathbf{R}=\\mathbf{I}\\) and \\(\\det(\\mathbf{R})=1\\).</li> <li>Euclidean transformations preserve Euclidean distances</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#similarity","title":"Similarity","text":"<p>Similarity: (2D Translation + Scaled 2D Rotation, 4 DoF)</p> \\[ \\mathbf{x}'=s\\mathbf{R}\\mathbf{x}+\\mathbf{t} \\Leftrightarrow \\begin{pmatrix}x'\\\\y'\\\\1\\end{pmatrix}=\\begin{bmatrix}s\\cos\\theta&amp;-s\\sin\\theta&amp;t_x\\\\s\\sin\\theta&amp;s\\cos\\theta&amp;t_y\\\\0&amp;0&amp;1\\end{bmatrix}\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix} \\Leftrightarrow \\mathbf{\\overline{x}}'=\\begin{bmatrix}s\\mathbf{R}&amp;\\mathbf{t}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\mathbf{\\overline{x}}\\] <ul> <li>\\(s\\) is the scale factor.</li> <li>The similarity transform preserves angles between lines</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#affine","title":"Affine","text":"<p>Affine: (2D Translation + 2D Rotation + 2D Shear, 6 DoF)</p> \\[ \\mathbf{x}'=\\mathbf{A}\\mathbf{x}+\\mathbf{t} \\Leftrightarrow \\begin{pmatrix}x'\\\\y'\\\\1\\end{pmatrix}=\\begin{bmatrix}a_{11}&amp;a_{12}&amp;t_x\\\\a_{21}&amp;a_{22}&amp;t_y\\\\0&amp;0&amp;1\\end{bmatrix}\\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix} \\Leftrightarrow \\mathbf{\\overline{x}}'=\\begin{bmatrix}\\mathbf{A}&amp;\\mathbf{t}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\mathbf{\\overline{x}}\\] <ul> <li>\\(\\mathbf{A}\\) is an arbitrary 2D matrix.</li> <li>Parallel lines remain parallel under affine transformations</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#projective","title":"Projective","text":"<p>Projective: (2D Translation + 2D Rotation + 2D Shear + 2D Homography, 8 DoF)\uff08\u5bf9\u540c\u6784\u77e9\u9635\u6574\u4f53\u4e58\u4ee5\u4e00\u4e2a\u5e38\u6570\u5e76\u4e0d\u6539\u53d8\u5c04\u5f71\u53d8\u6362,\u6240\u4ee5\u53ef\u4ee5\u8ba9\u5176\u4e2d\u4e00\u4e2a\u5143\u7d20\u53d6\u4efb\u610f\u5b9a\u503c(\u4f8b\u5982\u53d61),\u7b49\u4ef7\u53bb\u6389\u4e00\u4e2a\u81ea\u7531\u5ea6\uff09</p> \\[ \\mathbf{\\widetilde{x}}'=\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}} \\Leftrightarrow \\begin{pmatrix}\\widetilde{x}'\\\\\\widetilde{y}'\\\\\\widetilde{w}'\\end{pmatrix}=\\begin{bmatrix}h_{11}&amp;h_{12}&amp;h_{13}\\\\h_{21}&amp;h_{22}&amp;h_{23}\\\\h_{31}&amp;h_{32}&amp;h_{33}\\end{bmatrix}\\begin{pmatrix}\\widetilde{x}\\\\\\widetilde{y}\\\\\\widetilde{w}\\end{pmatrix} \\Leftrightarrow \\mathbf{\\overline{x}}'=\\frac{1}{\\widetilde{w}'}\\widetilde{x}'\\] <ul> <li>\\(\\mathbf{\\widetilde{H}}\\) is an arbitrary homogeneous 3x3 matrix.</li> <li>Projective transformations preserve straight lines</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#2d-transformations-on-co-vectors","title":"2D Transformations on Co-vectors","text":"<p>Consider a covector \\(\\mathbf{\\widetilde{l}}=(a,b,c)^T\\) representing a 2D line. How does it transform under a 2D transformation \\(\\mathbf{H}\\)?</p> <p>Answer: \\(\\mathbf{\\widetilde{l}'}=\\mathbf{H}^{-T}\\mathbf{\\widetilde{l}}\\)</p> <p>Solution: Consider a point \\(\\mathbf{\\overline{x}}\\) on the line \\(\\mathbf{\\widetilde{l}}\\):</p> <p>Considering any perspective 2D transformation: $$ \\mathbf{\\widetilde{x}}'=\\mathbf{H}\\mathbf{\\widetilde{x}}$$</p> <p>the transformed 2D line equation is given by: $$ \\mathbf{\\widetilde{l}'}^T\\mathbf{\\widetilde{x}}'=\\mathbf{\\widetilde{l}}^T\\mathbf{\\widetilde{x}}=0$$</p> <p>So: $$ \\mathbf{\\widetilde{l}'}^T\\mathbf{H}\\mathbf{\\widetilde{x}}=\\mathbf{\\widetilde{l}}^T\\mathbf{\\widetilde{x}}=0$$</p> <p>which implies: $$ \\mathbf{\\widetilde{l}'}=\\mathbf{H}^{-T}\\mathbf{\\widetilde{l}}$$</p> <p>Thus, the action of a projective transformation on a co-vector such as a 2D line or 3D normal can be represented by the transposed inverse of the matrix.</p>"},{"location":"CS/Note_02_Image_Formation/#214-overview-of-2d-transformations","title":"2.1.4 Overview of 2D Transformations","text":"Transformation DoF Preserves Matrix Translation 2 Orientation \\(\\begin{bmatrix}\\mathbf{I}&amp;\\mathbf{t}\\end{bmatrix}_{2\\times 3}\\) Rigid(Euclidean) 3 Length \\(\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}_{2\\times 3}\\) Similarity 4 Angles \\(\\begin{bmatrix}s\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}_{2\\times 3}\\) Affine 6 Parallelism \\(\\begin{bmatrix}\\mathbf{A}&amp;\\mathbf{t}\\end{bmatrix}_{2\\times 3}\\) Projective 8 Straight lines \\(\\begin{bmatrix}\\mathbf{\\widetilde{H}}\\end{bmatrix}_{3\\times 3}\\) <ul> <li>Interpret as restricted \\(3 \u00d7 3\\) matrices operating on 2D homogeneous coordinates</li> <li>Transformations preserve properties below</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#215-overview-of-3d-transformations","title":"2.1.5 Overview of 3D Transformations","text":"Transformation DoF Preserves Matrix Translation 3 Orientation \\(\\begin{bmatrix}\\mathbf{I}&amp;\\mathbf{t}\\end{bmatrix}_{3\\times 4}\\) Rigid(Euclidean) 6 Length \\(\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}_{3\\times 4}\\) Similarity 7 Angles \\(\\begin{bmatrix}s\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}_{3\\times 4}\\) Affine 12 Parallelism \\(\\begin{bmatrix}\\mathbf{A}&amp;\\mathbf{t}\\end{bmatrix}_{3\\times 4}\\) Projective 15 Straight lines \\(\\begin{bmatrix}\\mathbf{\\widetilde{H}}\\end{bmatrix}_{4\\times 4}\\) <ul> <li>3D transformations are defined analogously(\u7c7b\u4f3c\u7684) to 2D transformations</li> <li>\\(3 \u00d7 4\\) matrices are extended with a fourth \\(\\begin{bmatrix}\\mathbf{0}^T&amp;1\\end{bmatrix}\\) row for homogeneous transforms.</li> <li>Transformations preserve properties below (similarity: parallelism, straight lines)</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#216-direct-linear-transform-for-homography-estimation","title":"2.1.6 Direct Linear Transform for Homography Estimation","text":"<p>\u7528\u76f4\u63a5\u7ebf\u6027\u53d8\u6362(DLT)\u4f30\u8ba1\u5355\u5e94\u6027\u77e9\u9635(Homography Matrix)</p> <p>How can we estimate a homography from a set of 2D correspondences?</p> <ul> <li>2D correspondences: \\(\\mathbf{x}_i \\leftrightarrow \\mathbf{x}'_i\\)</li> <li>\\(\\mathbf{x}_i\\) is a point in the first image</li> <li>\\(\\mathbf{x}'_i\\) is the corresponding point in the second image</li> <li>Homography: \\(\\mathbf{\\widetilde{x}}'_i=\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i\\)</li> </ul> <p>Let \\(\\mathcal{X}=\\{\\mathbf{\\widetilde{x}_i,\\widetilde{x}'_i}\\}_{i=1}^N\\)denote a set of \\(N\\) 2D-to-2D correspondences related by \\(\\mathbf{\\widetilde{x}}'_i=\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i\\).As the correspondence vectors are homogeneous, they have the same direction but differ in magnitude.</p> <p>Why to say \"the same direction but differ in magnitude\"? See the following picture: </p> <p>Thus, the equation above can be expressed as \\(\\mathbf{\\widetilde{x}}'_i\\times\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i=0\\). Using the \\(\\mathbf{\\widetilde{h}}^T_k\\) to denote the k-th row of \\(\\mathbf{\\widetilde{H}}\\), we can write:</p> <p>Note:</p> <ol> <li>\\(\\mathbf{a}\\times\\mathbf{b}=\\begin{bmatrix}0&amp;-a_3&amp;a_2\\\\a_3&amp;0&amp;a_1\\\\-a_2&amp;a_1&amp;0\\end{bmatrix} \\begin{pmatrix}b_1\\\\b_2\\\\b_3\\end{pmatrix}=\\begin{pmatrix}a_2b_3-a_3b_2\\\\a_3b_1-a_1b_3\\\\a_1b_2-a_2b_1\\end{pmatrix}\\)</li> </ol> <p>\u5c55\u5f00\u53c9\u4e58\u8fd0\u7b97:</p> <p>\\(\\begin{aligned} &amp;\\mathbf{\\widetilde{x}}'_i \\times (\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i) = 0\\\\ \\Leftrightarrow &amp; \\begin{bmatrix} 0 &amp; -\\widetilde{w}'_i &amp; \\widetilde{y}'_i \\\\ \\widetilde{w}'_i &amp; 0 &amp; -\\widetilde{x}'_i\\\\ -\\widetilde{y}'_i &amp; \\widetilde{x}'_i &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\mathbf{\\widetilde{h}}^T_1\\\\ \\mathbf{\\widetilde{h}}^T_2\\\\ \\mathbf{\\widetilde{h}}^T_3 \\end{bmatrix}\\mathbf{\\widetilde{x}}_i = 0\\\\ \\Leftrightarrow &amp; \\begin{bmatrix} 0^T_{1\\times 3} &amp; -\\widetilde{w}'_i\\mathbf{\\widetilde{x}}_i^T &amp; \\widetilde{y}'_i\\mathbf{\\widetilde{x}}_i^T\\\\ \\widetilde{w}'_i\\mathbf{\\widetilde{x}}_i^T &amp; 0^T_{1\\times 3} &amp; -\\widetilde{x}'_i\\mathbf{\\widetilde{x}}_i^T\\\\ -\\widetilde{y}'_i\\mathbf{\\widetilde{x}}_i^T &amp; \\widetilde{x}'_i\\mathbf{\\widetilde{x}}_i^T &amp; 0^T_{1\\times 3} \\end{bmatrix}_{3\\times 9} \\begin{bmatrix} \\mathbf{\\widetilde{h}}_1\\\\ \\mathbf{\\widetilde{h}}_2\\\\ \\mathbf{\\widetilde{h}}_3 \\end{bmatrix}_{9\\times 1} = 0 \\end{aligned}\\)</p> <ul> <li>\u7b2c\u4e8c\u6b65\u5230\u7b2c\u4e09\u6b65\u662f\u56e0\u4e3a \\(\\mathbf{h}_i^T \\mathbf{\\widetilde{x}}_i\\)\u662f\u4e00\u4e2a\u6570\uff0c\u6240\u4ee5\\(\\mathbf{h}_i^T \\mathbf{\\widetilde{x} }_i = \\mathbf{\\widetilde{x} }_i^T \\mathbf{h}_i\\)\u3002</li> <li>\u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u5de6\u8fb9\u7684\u6570\u636e\u6765\u4f30\u8ba1 \\(\\mathbf{H}\\) \u4e86\u3002</li> <li>\u6700\u540e\u4e00\u884c\u7684\u4e09\u4e2a\u65b9\u7a0b\u662f\u5197\u4f59\u7684\uff0c\u56e0\u4e3a\\(-\\widetilde{w}'_iR_3 = \\widetilde{x}'_i R_1 + \\widetilde{y}'_i R_2\\)\u3002</li> </ul> <p>Each point correspondence yields two equations. Stacking all equations into a \\(2N \u00d7 9\\)(\u4e00\u7ec4\u6709\u4e24\u4e2a\u65b9\u7a0b\uff0c\u5217\u6570\u5747\u4e3a9) dimensional matrix \\(\\mathbf{A}\\) leads to the following constrained least squares problem</p> <p>\u7531\u4e8e\u8bef\u5dee\u7684\u5b58\u5728\uff0c\u4e0a\u5f0f\u53ef\u80fd\u4e0d\u7b49\u4e8e0\uff0c\u6240\u4ee5\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6765\u4f30\u8ba1 \\(\\mathbf{H}\\):</p> <p>\u4ee4 \\(\\mathbf{A} \\widetilde{\\mathbf{h}} = \\omega\\)</p> <p>$\\Omega = \\omega^T \\omega $</p> <p>\u6c42\u89e3 \\(\\widetilde{\\mathbf{h}}\\) \u4f7f\u5f97 \\(\\Omega\\) \u6700\u5c0f\u4e5f\u5c31\u662f</p> \\[ \\widetilde{\\mathbf{h}}^*=\\arg\\min_{\\widetilde{\\mathbf{h}}}\\Omega + \\lambda (\\|\\widetilde{\\mathbf{h}}\\|^2-1)=\\arg\\min_{\\widetilde{\\mathbf{h}}}\\|\\mathbf{A}\\widetilde{\\mathbf{h}}\\|^2+\\lambda (\\|\\widetilde{\\mathbf{h}}\\|^2-1)\\] <p>\u5176\u4e2d \\(\\widetilde{\\mathbf{h}}^*\\) \u662f \\(\\mathbf{H}\\) \u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\u3002</p> <p>where \\(\\lambda\\) is a Lagrange multiplier(\u62c9\u683c\u6717\u65e5\u4e58\u5b50) to enforce the constraint \\(\\|\\widetilde{\\mathbf{h}}\\|^2=1\\).</p> <p>The solution to the above optimization problem is the singular vector corresponding to the smallest singular value of \\(\\mathbf{A}\\).(i.e., the last column of \\(V\\) when decomposing \\(A = UDV^T\\) , see also Deep Learning lecture 11.2). The resulting algorithm is called Direct Linear Transformation.</p>"},{"location":"CS/Note_02_Image_Formation/#22-geometric-image-formation","title":"2.2 Geometric Image Formation","text":""},{"location":"CS/Note_02_Image_Formation/#221-basic-camera-models","title":"2.2.1 Basic camera models","text":"<p>Physical Camera Model </p> <p>Mathematical Camera Model </p>"},{"location":"CS/Note_02_Image_Formation/#222-projection-models","title":"2.2.2 Projection models","text":""},{"location":"CS/Note_02_Image_Formation/#orthographic-projection","title":"Orthographic Projection*","text":"<p>An orthographic projection simply drops the z component of the 3D point in camera coordinates \\(\\mathbf{x}_c\\) to obtain the corresponding 2D point on the image plane (= screen) \\(\\mathbf{x}_s\\).</p> <p></p> \\[ \\mathbf{x}_s=\\begin{bmatrix}1&amp;0&amp;0\\\\0&amp;1&amp;0\\end{bmatrix}\\mathbf{x}_c \\Leftrightarrow \\overline{\\mathbf{x}}_s=\\begin{bmatrix}1&amp;0&amp;0&amp;0\\\\0&amp;1&amp;0&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\overline{\\mathbf{x}}_c\\] <ul> <li>remember that \\(\\overline{\\mathbf{x}}_c\\) is the augmented vector of \\(\\mathbf{x}_c\\).</li> <li>Orthography is exact for telecentric lenses and an approximation for telephoto lenses. After projection the distance of the 3D point from the image can\u2019t be recovered</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#scaled-orthographic-projection","title":"Scaled Orthographic Projection","text":"<p>In practice, world coordinates (which may measure dimensions in meters) must be scaled to fit onto an image sensor (measuring in pixels) \u21d2 scaled orthography:</p> \\[ \\mathbf{x}_s=\\begin{bmatrix}s&amp;0&amp;0\\\\0&amp;s&amp;0\\end{bmatrix}\\mathbf{x}_c \\Leftrightarrow \\overline{\\mathbf{x}}_s=\\begin{bmatrix}s&amp;0&amp;0&amp;0\\\\0&amp;s&amp;0&amp;0\\\\0&amp;0&amp;0&amp;1\\end{bmatrix}\\overline{\\mathbf{x}}_c\\] <p>The unit for s is px/m or px/mm to convert metric 3D points into pixels</p> <p>Under orthography, structure and motion can be estimated simultaneously using factorization methods (e.g., via singular value decomposition).</p>"},{"location":"CS/Note_02_Image_Formation/#perspective-projection","title":"Perspective Projection**","text":"<p>For perspective projection, we can use the following relationship (which is just from the principle of equal triangles): $$ \\frac{x_s}{f}=\\frac{x_c}{z_c}$$</p> <p></p> <p>In perspective projection, 3D points in camera coordinates are mapped to the image plane by dividing them by their z component and multiplying with the focal length:</p> \\[ \\begin{pmatrix}x_s\\\\y_s\\end{pmatrix}=\\begin{pmatrix}fx_c/z_c\\\\fy_c/z_c\\end{pmatrix} \\Leftrightarrow \\widetilde{\\mathbf{x}}_s=\\begin{bmatrix}f&amp;0&amp;0\\\\0&amp;f&amp;0\\\\0&amp;0&amp;1\\end{bmatrix}\\overline{\\mathbf{x}}_c\\] <ul> <li>Note that this projection is linear when using homogeneous coordinates. After the projection it is not possible to recover the distance of the 3D point from the image.</li> <li>Remark: The unit for \\(f\\) is px (=pixels) to convert metric 3D points into pixels.</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#principal-point-offset","title":"Principal Point Offset","text":"<p>Usually in practice, we compute the principal point offset to derive a coordinate system for the image plane that is much more convenient as it does not include negative values.</p> <ol> <li> <p>Without principal point offset </p> </li> <li> <p>With principal point offset </p> </li> </ol> <p>This moves the image coordinate system to the corner of the image plane</p>"},{"location":"CS/Note_02_Image_Formation/#complete-perspective-projection-model","title":"Complete perspective projection model","text":"<p>The complete perspective projection model is given by:</p> \\[ \\begin{pmatrix}x_s\\\\y_s\\end{pmatrix}=\\begin{pmatrix}f_xx_c/z_c+sy_c+c_x\\\\f_yy_c/z_c+c_y\\end{pmatrix} \\Leftrightarrow \\widetilde{\\mathbf{x}}_s=\\begin{bmatrix}f_x&amp;s&amp;c_x&amp;0\\\\0&amp;f_y&amp;c_y&amp;0\\\\0&amp;0&amp;1&amp;0\\end{bmatrix}\\overline{\\mathbf{x}}_c\\] <ul> <li>The Matrix \\(\\mathbf{K}=\\begin{bmatrix}f_x&amp;s&amp;c_x\\\\0&amp;f_y&amp;c_y\\\\0&amp;0&amp;1\\end{bmatrix}\\) is called the calibration matrix.</li> <li>The parameters of K are called camera intrinsics (as opposed to extrinsic pose)</li> <li>Here, \\(f_x\\) and \\(f_y\\) are independent, allowing for different pixel aspect ratios</li> <li>The skew parameter \\(s\\) arises due to the sensor not mounted perpendicular to the optical axis</li> <li>In practice, we often set \\(s=0\\) and \\(f_x=f_y=f\\) to simplify the model</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#chaining-transformations","title":"Chaining Transformations","text":"<p>Let \\(\\mathbf{K}\\) be the calibration matrix (intrinsics) and \\(\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}\\) be the camera pose (extrinsics). We chain both transformations to project a point in world coordinates to the image:</p> \\[ \\widetilde{\\mathbf{x}}_s=\\begin{bmatrix}\\mathbf{K}&amp;\\mathbf{0}\\end{bmatrix}\\overline{\\mathbf{x}}_c=\\begin{bmatrix}\\mathbf{K}&amp;\\mathbf{0}\\end{bmatrix}\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\overline{\\mathbf{x}}_w=\\mathbf{K}\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}\\overline{\\mathbf{x}}_w=\\mathbf{P}\\overline{\\mathbf{x}}_w\\] <p></p> <p>The matrix \\(\\mathbf{P}=\\mathbf{K}\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\end{bmatrix}\\) is called the camera matrix, which is a \\(3 \u00d7 4\\) matrix, can be precomputed and is used to project 3D points to the image plane.</p>"},{"location":"CS/Note_02_Image_Formation/#full-rank-representation","title":"Full Rank Representation","text":"<p>It is sometimes preferable to use a full rank 4 \u00d7 4 projection matrix:</p> \\[ \\widetilde{\\mathbf{x}}_s=\\begin{bmatrix}\\mathbf{K}&amp;\\mathbf{0}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\begin{bmatrix}\\mathbf{R}&amp;\\mathbf{t}\\\\\\mathbf{0}^T&amp;1\\end{bmatrix}\\overline{\\mathbf{x}}_w\\] <p>Now, the homogeneous vector \\(\\widetilde{\\mathbf{x}}_s\\) is a 4D vector and must be normalized wrt. its 3rd entry to obtain inhomogeneous image pixels: $$ \\overline{\\mathbf{x}}_s=\\frac{\\widetilde{\\mathbf{x}}_s}{\\widetilde{z}_s}= \\begin{pmatrix}x_s/z_s &amp; y_s/z_s &amp; 1 &amp; 1/z_s\\end{pmatrix}^T$$ Note that the 4th component of the inhomogeneous 4D vector is the inverse depth. If the inverse depth is known, a 3D point can be retrieved from its pixel coordinates via \\(\\widetilde{\\mathbf{x}}_w=\\mathbf{P}^{-1}\\overline{\\mathbf{x}}_s\\) and subsequent normalization of \\(\\widetilde{\\mathbf{x}}\\) wrt. its 4th entry.</p>"},{"location":"CS/Note_02_Image_Formation/#223-lens-distortion","title":"2.2.3 Lens Distortion","text":"<p>The assumption of linear projection (straight lines remain straight) is violated in practice due to the properties of the camera lens which introduces distortions. Both radial and tangential distortion effects can be modeled relatively easily: Let \\(x = x_c/z_c, y = y_c/z_c\\) and \\(r^2 = x^2 + y^2\\) . The distorted point is obtained as:</p> \\[ \\mathbf{x}' = (1+\\kappa_1r^2+\\kappa_2r^4)\\begin{pmatrix}x\\\\y\\end{pmatrix}+\\begin{pmatrix}2\\tau_1xy+\\tau_2(r^2+2x^2)\\\\2\\tau_2xy+\\tau_1(r^2+2y^2)\\end{pmatrix}\\] <p>where \\(\\kappa_1, \\kappa_2\\) are the radial distortion coefficients and \\(\\tau_1, \\tau_2\\) are the tangential distortion coefficients.</p> <p>Images can be undistorted such that the perspective projection model applies. Note here, that is practice, this is already done before the image is computed. More complex distortion models must be used for wide-angle lenses</p>"},{"location":"CS/Note_02_Image_Formation/#23-photometric-image-formation","title":"2.3 Photometric Image Formation","text":"<p>We now discuss how an image is formed in terms of pixel intensities and colors.</p>"},{"location":"CS/Note_02_Image_Formation/#231-rendering-equation","title":"2.3.1 Rendering Equation","text":"<p>Let \\(\\mathbf{p} \\in \\mathbb{R}^3\\) denote  a 3D surface point, \\(\\mathbf{v} \\in \\mathbb{R}^3\\) the viewing direction and \\(\\mathbf{s} \\in \\mathbb{R}^3\\) the incoming light direction. The rendering equation describes how much of the light \\(L_{in}\\) with wavelength \\(\\lambda\\) arriving at \\(\\mathbf{p}\\) is reflected into the viewing direction \\(\\mathbf{v}\\):</p> \\[L_{out}(\\mathbf{p},\\mathbf{v},\\lambda)=L_{emit}(\\mathbf{p},\\mathbf{v},\\lambda)+\\int_{\\Omega}BRDF(\\mathbf{p},\\mathbf{v},\\mathbf{s},\\lambda)L_{in}(\\mathbf{p},\\mathbf{s},\\lambda)(-\\mathbf{n}^T\\mathbf{s})d\\mathbf{s}\\] <ul> <li>\\(\\Omega\\) is the unit hemisphere at normal \\(\\mathbf{n}\\).</li> <li>\\(BRDF\\) is the bidirectional reflectance distribution function defines how light is reflected at an opaque surface(\u4e0d\u900f\u660e\u8868\u9762), \u8868\u793a\u4ece\u65b9\u5411\\(\\mathbf{s}\\)\u5165\u5c04\u7684\u5149\u6709\u591a\u5c11\u6bd4\u4f8b\u4f1a\u53cd\u5c04\u5230\u65b9\u5411\\(\\mathbf{v}\\)</li> <li>\\(L_{emit} &gt; 0\\) only for light emitting surfaces, \u63cf\u8ff0\u7684\u662f\u70b9p\u4f5c\u4e3a\u5149\u6e90\u81ea\u8eab\u53d1\u51fa\u7684\u5149,\u4e0d\u4f9d\u8d56\u4e8e\u5916\u754c\u7684\u5165\u5c04\u5149\u3002\u6bd4\u5982\u4e00\u4e2a\u53d1\u5149\u7684\u7403\u4f53\u8868\u9762,\u5b83\u81ea\u8eab\u5c31\u4f1a\u53d1\u51fa\u5149\u7ebf\u3002</li> <li>\\((-\\mathbf{n}^T\\mathbf{s})\\) attenuation equation (if light arrives exactly perpendicular, there is no reflectance at all. or if it arrives at a shallow angle, there is less light reflected).</li> <li>\u6839\u636eLambert cosine law,\u4e00\u4e2a\u8868\u9762\u7684\u53cd\u5c04\u4eae\u5ea6\u4e0e\u5149\u6e90\u5165\u5c04\u89d2\u7684\u4f59\u5f26\u503c\u6210\u6b63\u6bd4\u3002</li> <li>\u5176\u5b9e\u5c31\u662f\\(\\mathbf{n}\\)\u548c\\(\\mathbf{s}\\)\u7684\u5939\u89d2\u7684\u4f59\u5f26\u503c\u3002</li> <li>\u4f59\u5f26\u503c\u6b63\u53f7\u8868\u793a\u5149\u7167\u65b9\u5411\u548c\u6cd5\u7ebf\u540c\u5411,\u8d1f\u53f7\u8868\u793a\u5149\u7167\u65b9\u5411\u548c\u6cd5\u7ebf\u53cd\u5411\u3002\u52a0\u5165\u8d1f\u53f7\u662f\u4e3a\u4e86\u8ba1\u7b97\u5149\u7167\u4ece\u80cc\u9762\u5165\u5c04\u65f6\u7684\u6b63\u786e\u6548\u679c</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#brdf-components","title":"BRDF Components","text":"<p>Typical BRDFs have a diffuse(\u6f2b\u53cd\u5c04) and a specular(\u955c\u9762\u53cd\u5c04) component:</p> <p></p> <ul> <li>\u56e0\u4e3a\u8868\u9762\u4e0d\u4e00\u5b9a\u662f\u5b8c\u5168\u5149\u6ed1\u7684,\u6240\u4ee5\u4f1a\u5448\u73b0Specular\u7684\u6548\u679c</li> <li>The specular component depends strongly on the outgoing light direction.\uff08\u60f3\u60f3\u5851\u6599\u7684\u5404\u4e2a\u89d2\u5ea6\u7684\u6548\u679c\uff09</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#232-brdf-in-practice","title":"2.3.2 BRDF in practice","text":""},{"location":"CS/Note_02_Image_Formation/#fresnel-effect","title":"Fresnel Effect","text":"<p>The specular component can get stronger if the surface is further away because the viewing angle changes (example with the water reflectance). So the amount of relectance depends on the viewing angle.</p>"},{"location":"CS/Note_02_Image_Formation/#global-illumination","title":"Global Illumination","text":"<p>Modeling one light bounce is insufficient for rendering complex scenes. Light sources can be shadowed by occluders and rays can bounce multiple times. Global illumination techniques also take indirect illumination into account.</p>"},{"location":"CS/Note_02_Image_Formation/#233-camera-lenses","title":"2.3.3 Camera lenses","text":""},{"location":"CS/Note_02_Image_Formation/#the-thin-lens-model","title":"The thin lens model","text":"\\[ \\frac{1}{f}=\\frac{1}{z_s}+\\frac{1}{z_c}\\] <ul> <li>\\(f\\) is the focal length</li> <li>\\(z_s\\) is the distance from the lens to the image plane</li> <li>\\(z_c\\) is the distance from the lens to the object plane</li> </ul> <p>The thin lens model with spherical lens is often used as an approximation. Properties: Axis-parallel rays pass the focal point, rays via center keep direction. From Snell\u2019s law we obtain \\(f=\\frac{R}{2(n-1)}\\) with radius \\(R\\) and index of refraction \\(n\\).</p>"},{"location":"CS/Note_02_Image_Formation/#depth-of-field","title":"Depth of Field(\u666f\u6df1)","text":"<p>\u8fd9\u91cc\u7684\\(\\Delta z_s\\)\u662f\u6307\u5728\u7126\u5e73\u9762\u4e0a\u7684\u666f\u6df1\u8303\u56f4\uff0c\u662f\u524d\u666f\u548c\u540e\u666f\u7684\u8ddd\u79bb\u3002</p> <p>If the image plane is out of focus, a 3D point projects to the circle of confusion \\(c\\). The circle of confusion is a little disc - if the disc is larger than the pixel size, we get blur!</p> <p>The allowable depth variation that limits the circle of confusion \\(c\\) is called depth of field and is a function of both the focus distance and the lens aperture. Typical DSLR cameras have a Depth of Field Indicator which describes the range where the image appears sharply. The commonly displayed f-number is defined as $$ N = \\frac{f}{d}$$ where \\(f\\) is the focal length and \\(d\\) is the diameter of aperture\uff08\u5149\u5708\u76f4\u5f84\uff09. The f-number is inversely proportional to the aperture diameter.</p> <p>To control the size of the circle of confusion, we change the lens aperture. An aperture is a hole or an opening through which light travels. The aperture limits the amount of light that can reach the image plane. Smaller apertures lead to sharper, but more noisy images (less photons)</p>"},{"location":"CS/Note_02_Image_Formation/#chromatic-aberration","title":"Chromatic Aberration(\u8272\u5dee)","text":""},{"location":"CS/Note_02_Image_Formation/#vignetting","title":"Vignetting(\u6655\u5f71)","text":""},{"location":"CS/Note_02_Image_Formation/#24-image-sensing-pipeline","title":"2.4 Image Sensing Pipeline","text":"<p>The image sensing pipeline can be divided into three stages:</p> <ul> <li>Physical light transport in the camera lens/body</li> <li>Optics(\u5149\u5b66) + Aperture(\u5149\u5708) + Shutter(\u5feb\u95e8)</li> <li>Photon measurement and conversion on the sensor chip(\u4f20\u611f\u5668\u82af\u7247)</li> <li>Sensor(\u4f20\u611f\u5668) + Gain(ISO)(\u589e\u76ca) + ADC(\u6a21\u6570\u8f6c\u6362\u5668)</li> <li>Image signal processing (ISP) and image compression</li> <li>Demosaic(\u53bb\u9a6c\u8d5b\u514b) + Denoise and sharpen(\u53bb\u566a\u548c\u9510\u5316) + White balance(\u767d\u5e73\u8861) + Gamma/curve(\u4f3d\u9a6c/\u66f2\u7ebf) + Compress(\u538b\u7f29)</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#241-shutter","title":"2.4.1 Shutter","text":"<p>A focal plane shutter(\u7126\u5e73\u9762\u5feb\u95e8) is positioned just in front the image sensor / film. Most digital cameras use a combination of mechanical and electronic shutter. The shutter speed (exposure time) controls how much light reaches the sensor It determines if an image appears over- (too light)/underexposed (too dark), blurred (motion blur) or noisy.</p>"},{"location":"CS/Note_02_Image_Formation/#242-sensor","title":"2.4.2 Sensor","text":"<p>Two main principles: CCD and CMOS for light sensors. CCDs move charge from pixel to pixel and convert it to voltage at the output node. CMOS images convert charge to voltage inside each pixel and are standard today</p>"},{"location":"CS/Note_02_Image_Formation/#243-color-filter-arrays","title":"2.4.3 Color Filter Arrays","text":"<p>To measure color, pixels are arranged in a color array, e.g.: Bayer RGB pattern. Missing colors at each pixel are interpolated from the neighbors (demosaicing)</p> <p>Each pixel integrates the light spectrum \\(L\\) according to its spectral sensitivity \\(S\\):</p> \\[ \\mathbf{R}=\\int_{\\lambda}L(\\lambda)S_R(\\lambda)d\\lambda\\]"},{"location":"CS/Note_02_Image_Formation/#244-different-color-spaces","title":"2.4.4 Different color spaces","text":"<p>RGB: red, green, blue Lab*: lightness, red-green, blue-yellow HSV: hue(\u8272\u8c03), saturation(\u9971\u548c\u5ea6), value(\u660e\u5ea6)</p>"},{"location":"CS/Note_02_Image_Formation/#245-gamma-compression","title":"2.4.5 Gamma Compression(\u4f3d\u9a6c\u538b\u7f29)","text":"<ul> <li>Humans are more sensitive to intensity differences in darker regions</li> <li>Therefore, it is beneficial to nonlinearly transform (left) the intensities or colors prior to discretization (left) and to undo this transformation during loading</li> </ul>"},{"location":"CS/Note_02_Image_Formation/#246-image-compression","title":"2.4.6 Image Compression","text":"<ul> <li>Often images are compressed into a format similar to JPEG.</li> <li>Typically luminance is compressed with higher fidelity than chrominance.</li> <li>Often, (8 \u00d7 8 pixel) patch-based discrete cosine or wavelet transforms are used.</li> <li>-Discrete Cosine Transform (DCT) is an approximation to PCA on natural images.</li> <li>The coefficients are quantized to integers that can be stored with Huffman codes.</li> <li>More recently, deep network based compression algorithms are developed (improving the compression a lot compared to DCT)</li> </ul>"},{"location":"CS/Note_03_Structure_from_Motion/","title":"3 Structure from Motion","text":"<p>This lecture builds on the concepts from the second lecture to reason about the 3D structure of a static scene from multiple images observing that scene. (\u672c\u8bb2\u5ea7\u5efa\u7acb\u5728\u7b2c\u4e8c\u8bb2\u7684\u6982\u5ff5\u4e4b\u4e0a\uff0c\u4ece\u591a\u4e2a\u89c2\u5bdf\u573a\u666f\u7684\u56fe\u50cf\u63a8\u65ad\u9759\u6001\u573a\u666f\u7684\u4e09\u7ef4\u7ed3\u6784\u3002)</p>"},{"location":"CS/Note_03_Structure_from_Motion/#31","title":"3.1 \u9884\u5148\u51c6\u5907","text":""},{"location":"CS/Note_03_Structure_from_Motion/#311-camera-calibration","title":"3.1.1 Camera Calibration(\u76f8\u673a\u6807\u5b9a)","text":"<p>To infer 3D-information from a collection of (2D) images, it is important to know the intrinsics and extrinsics of the camera setup.</p> <p>The process of finding the intrinsic and extrinsic parameters is known as camera calibration. Most commonly, a known calibration target is used, such as an image or a checkerboard.</p> <ul> <li>First, this target is captured in different poses. </li> <li>Then, features, such as corners, are detected in the images. </li> <li>Finally, the camera intrinsics and extrinsics are jointly optimized by non-linear optimization of reprojection errors.</li> <li>Closed-form solution initializes all parameters except for distortion parameters</li> <li> <p>Non-linear optimization of all parameters by minimizing reprojection errors </p> </li> <li> <p>There exists a variety of calibration techniques that are used in different settings.</p> </li> <li>These methods differ algorithmically, but also in the type of assumptions and calibration targets they use: 2D/3D targets, planes, vanishing points, etc.</li> </ul> <p>Further Readings:</p> <ul> <li>Camera Calibration Toolbox for Matlab</li> <li>OpenCV Camera Calibration</li> <li>Szeliski Book, Chapter 11.1</li> </ul>"},{"location":"CS/Note_03_Structure_from_Motion/#312-feature-detection-and-description","title":"3.1.2 Feature Detection and Description(\u7279\u5f81\u68c0\u6d4b\u4e0e\u63cf\u8ff0)","text":""},{"location":"CS/Note_03_Structure_from_Motion/#point-features","title":"Point Features","text":"<ul> <li> <p>Point features describe the appearance of local, salient regions in an image. They can be used to describe and match images taken from different viewpoints</p> </li> <li> <p>Features should be invariant to perspective effects and illumination and the same point should have similar vectors independent of pose or viewpoint. Plain RGB / intensity patches will not have this property, we need something better.</p> </li> </ul> <p>As an example, think of the RGB matrices that describe one image and the same exact image rotated 180\u25e6. The matrices will be completely different, although they describe the exact same set of points.</p>"},{"location":"CS/Note_03_Structure_from_Motion/#scale-invariant-feature-transform-sift","title":"Scale Invariant Feature Transform (SIFT)(\u5c3a\u5ea6\u4e0d\u53d8\u7279\u5f81\u53d8\u6362)","text":"<p>SIFT was a seminal work due to its invariance and robustness, which revolutionized recognition and in particular matching and enabled the development of large-scale SfM techniques.</p> <p>SIFT\u7b97\u6cd5\u8be6\u89e3(CSDN) One algorithm that provides us with what we need is the Scale Invariant Feature Transform (SIFT).\u5b83\u5728\u7a7a\u95f4\u5c3a\u5ea6\u4e2d\u5bfb\u627e\u6781\u503c\u70b9\uff0c\u5e76\u63d0\u53d6\u51fa\u5176\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65cb\u8f6c\u4e0d\u53d8\u91cf\u3002</p> <p></p> <ul> <li> <p>SIFT constructs a scale space by iteratively filtering the image with a Gaussian and scaling the image down at regular intervals.</p> </li> <li> <p>Adjacent scales are subtracted, yielding Difference of Gaussian (DoG) images.</p> </li> <li> <p>\u5de6\u56fe\u5bf9\u76f8\u90bb\u7684\u4e24\u4e2a\u9ad8\u65af\u56fe\u50cf\u505a\u5dee\uff0c\u5f97\u5230\u7684\u56fe\u50cf\u79f0\u4e3a\u9ad8\u65af\u5dee\u5206\u56fe\u50cf\uff09</p> </li> <li>Difference of Gaussian filters are \u201dblob detectors\u201d; the interest points (blobs) are detected as extrema in the resulting scale space.\uff08</li> <li> <p>\u53f3\u56fe\u6b63\u5728\u8fdb\u884c\u6781\u503c\u70b9\u68c0\u6d4b\uff0c\u4e2d\u95f4\u7684\u68c0\u6d4b\u70b9\u548c\u5b83\u540c\u5c3a\u5ea6\u76848\u4e2a\u76f8\u90bb\u70b9\u548c\u4e0a\u4e0b\u76f8\u90bb\u5c3a\u5ea6\u5bf9\u5e94\u76849\u00d72\u4e2a\u70b9\u517126\u4e2a\u70b9\u6bd4\u8f83\uff0c\u4ee5\u786e\u4fdd\u5728\u5c3a\u5ea6\u7a7a\u95f4\u548c\u4e8c\u7ef4\u56fe\u50cf\u7a7a\u95f4\u90fd\u68c0\u6d4b\u5230\u6781\u503c\u70b9\u3002\uff09</p> </li> <li> <p>\u7531\u4e8e\u8981\u5728\u76f8\u90bb\u5c3a\u5ea6\u8fdb\u884c\u6bd4\u8f83\uff0c\u5982\u56fe3.3\u53f3\u4fa7\u6bcf\u7ec4\u542b4\u5c42\u7684\u9ad8\u65af\u5dee\u5206\u91d1\u5b50\u5854\uff0c\u53ea\u80fd\u5728\u4e2d\u95f4\u4e24\u5c42\u4e2d\u8fdb\u884c\u4e24\u4e2a\u5c3a\u5ea6\u7684\u6781\u503c\u70b9\u68c0\u6d4b\uff0c\u5176\u5b83\u5c3a\u5ea6\u5219\u53ea\u80fd\u5728\u4e0d\u540c\u7ec4\u4e2d\u8fdb\u884c\u3002\u4e3a\u4e86\u5728\u6bcf\u7ec4\u4e2d\u68c0\u6d4bS\u4e2a\u5c3a\u5ea6\u7684\u6781\u503c\u70b9\uff0c\u5219DOG\u91d1\u5b57\u5854\u6bcf\u7ec4\u9700S+2\u5c42\u56fe\u50cf\uff0c\u800cDOG\u91d1\u5b57\u5854\u7531\u9ad8\u65af\u91d1\u5b57\u5854\u76f8\u90bb\u4e24\u5c42\u76f8\u51cf\u5f97\u5230\uff0c\u5219\u9ad8\u65af\u91d1\u5b57\u5854\u6bcf\u7ec4\u9700S+3\u5c42\u56fe\u50cf\uff0c\u5b9e\u9645\u8ba1\u7b97\u65f6S\u57283\u52305\u4e4b\u95f4\u3002</p> </li> <li> <p>\u5f53\u7136\u8fd9\u6837\u4ea7\u751f\u7684\u6781\u503c\u70b9\u5e76\u4e0d\u5168\u90fd\u662f\u7a33\u5b9a\u7684\u7279\u5f81\u70b9\uff0c\u56e0\u4e3a\u67d0\u4e9b\u6781\u503c\u70b9\u54cd\u5e94\u8f83\u5f31\uff0c\u800c\u4e14DOG\u7b97\u5b50\u4f1a\u4ea7\u751f\u8f83\u5f3a\u7684\u8fb9\u7f18\u54cd\u5e94</p> </li> </ul> <p></p> <ul> <li>After extracting the interest points, SIFT rotates the descriptor to align with the dominant gradient orientation.</li> <li>Then, gradient histograms are computed for local sub-regions of the descriptor which are concatenated and normalized to form a \\(128D\\) feature vector (the keypoint descriptor).</li> </ul> <p>\u8fd9128\u7ef4\u7684\u7279\u5f81\u5411\u91cf\u7531128\u7ef4\u7684\u5c40\u90e8\u7279\u5f81\u5411\u91cf\u8fde\u63a5\u800c\u6210\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u7279\u5f81\u5411\u91cf\u7684\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u5c06\u4e3b\u65b9\u5411\u4f5c\u4e3a\u53c2\u8003\u5750\u6807\u7cfb\uff0c\u5bf9\u5750\u6807\u7cfb\u8fdb\u884c\u65cb\u8f6c\uff0c\u4f7f\u5f97\u65cb\u8f6c\u540e\u7684\u5750\u6807\u7cfb\u4e0e\u6c34\u5e73\u65b9\u5411\u4fdd\u6301\u4e00\u81f4\uff0c\u7136\u540e\u5c06\u65cb\u8f6c\u540e\u7684\u5750\u6807\u7cfb\u5206\u621044\u7684\u5c0f\u65b9\u683c\uff0c\u7edf\u8ba1\u6bcf\u4e2a\u5c0f\u65b9\u683c\u5185\u7684\u68af\u5ea6\u76f4\u65b9\u56fe\uff0c\u6bcf\u4e2a\u5c0f\u65b9\u683c\u51858\u4e2a\u65b9\u5411\u7684\u68af\u5ea6\u76f4\u65b9\u56fe\u7ec4\u6210\u4e00\u4e2a8\u7ef4\u7684\u5c40\u90e8\u7279\u5f81\u5411\u91cf\uff0c44\u4e2a\u5c0f\u65b9\u683c\u517132\u4e2a\u5c40\u90e8\u7279\u5f81\u5411\u91cf\u7ec4\u6210128\u7ef4\u7684SIFT\u7279\u5f81\u5411\u91cf\u3002</p> <ul> <li>These operations make the descriptor invariant to rotation and brightness changes.</li> </ul> <p>The feature vectors constructed by SIFT can be used to find matching points in two or more images by efficient nearest neighbor search algorithms. Ambiguous matches are typically \ufb01ltered by computing the ratio of distance from the closest neighbor to the distance of the second closest; a large ratio (&gt; 0.8) indicates that the found match might not be the correct one.</p>"},{"location":"CS/Note_03_Structure_from_Motion/#32-two-frame-structure-from-motion","title":"3.2 Two-frame Structure-from-Motion","text":""},{"location":"CS/Note_03_Structure_from_Motion/#321-epipolar-geometry","title":"3.2.1 Epipolar Geometry(\u6781\u7ebf\u51e0\u4f55)","text":"<p>Goal: Recovery of camera pose (and 3D structure) from image correspondences</p> <p>The required relationships are described by the two-view epipolar geometry.</p> <p></p> <ul> <li>In the illustration, the rotation matrix \\(\\mathbf{R}\\) and the translation vector \\(\\mathbf{t}\\) denote the relative pose between two perspective cameras.</li> <li>A 3D point \\(\\mathbf{x}\\) is projected to pixel \\(\\bar{\\mathbf{x}}_1\\) in image 1 and to pixel \\(\\bar{\\mathbf{x}}_2\\) in image 2.</li> <li>The 3D point \\(\\mathbf{x}\\) and the two camera centers span the epipolar plane, on which also the two points \\(\\bar{\\mathbf{x}}_1\\) and \\(\\bar{\\mathbf{x}}_2\\) lie.</li> <li>The correspondence of pixel \\(\\bar{\\mathbf{x}}_1\\) in image 2 must lie on the epipolar line \\(\\bar{l}_2\\) in image 2.</li> </ul> <p>This means that, if the epipolar plane is known (i.e. \\(\\mathbf{R}\\) and \\(\\mathbf{t}\\) as well as camera intrinsics are known), to find the matching point \\(\\bar{\\mathbf{x}}_2\\) to \\(\\bar{\\mathbf{x}}_1\\), one only has to search along a one-dimensional search space (the epipolar line \\(\\bar{l}_2\\)).</p> <ul> <li>Finally, all epipolar lines must pass though the epipoles, which are the points on the image planes where the baseline (the connection of the two camera centers) passes though the image planes (this point can also lie at infinity).</li> </ul>"},{"location":"CS/Note_03_Structure_from_Motion/#322-estimate-epipolar-geometry","title":"3.2.2 Estimate epipolar geometry","text":"<p>The challenge now is to estimate the epipolar geometry based on detected matching features in the two images. As noted above, the inverse problem (matching features when knowing the epipolar geometry) is harder.(\u6240\u4ee5\u6211\u4eec\u5148\u6709\u70b9\uff0c\u7136\u540e\u8ba1\u7b97\u6781\u7ebf\u51e0\u4f55)</p> <p>Let us assume the camera matrices \\((\\mathbf{K}_i \\in \\mathbb{R}^{3 \\times 3})_{i=1}^2\\) are known, for example through calibration (Sec. 3.1.1). Let \\(\\tilde{\\mathbf{x}}_i = \\mathbf{K}_i^{-1} \\bar{\\mathbf{x}}_i\\) denote the local ray direction of pixel \\(\\bar{\\mathbf{x}}_i\\) in camera \\(i\\). Then we have:</p> \\[ \\tilde{\\mathbf{x}}_2 \\propto \\mathbf{x}_2 = \\mathbf{R} \\mathbf{x}_1 + \\mathbf{t} \\propto \\mathbf{R} \\tilde{\\mathbf{x}}_1 + s\\mathbf{t} \\] <p>\u56e0\u4e3a\\(\\tilde{\\mathbf{x}}_i\\)\u548c\\(\\mathbf{x}_i\\)\u662f\u540c\u4e00\u6761\u5c04\u7ebf\uff0c\u6240\u4ee5\\(\\tilde{\\mathbf{x}}_i\\)\u548c\\(\\mathbf{x}_i\\)\u662f\u6210\u6bd4\u4f8b\u7684\uff0c\u5373\\(\\tilde{\\mathbf{x}}_i \\propto \\mathbf{x}_i\\)\uff0c\u6240\u4ee5\\(\\tilde{\\mathbf{x}}_2 \\propto \\mathbf{x}_2\\)\u3002</p> <p>\u5bf9\u4e0a\u5f0f\u5de6\u53c9\u4e58\\(\\mathbf{t}\\)\uff0c\u5f97\u5230\uff1a</p> \\[ \\mathbf{t} \\times \\tilde{\\mathbf{x}}_2 \\propto \\mathbf{t} \\times (\\mathbf{R} \\tilde{\\mathbf{x}}_1+ s\\mathbf{t}) = \\mathbf{t} \\times \\mathbf{R} \\tilde{\\mathbf{x}}_1 \\] <p>\u5373\uff1a</p> \\[ [\\mathbf{t}]_\\times \\tilde{\\mathbf{x}}_2 \\propto [\\mathbf{t}]_\\times \\mathbf{R} \\tilde{\\mathbf{x}}_1 \\] <p>\u5bf9\u4e0a\u5f0f\u5de6\u70b9\u4e58\\(\\tilde{\\mathbf{x}}^T_2\\)\uff0c\u5f97\u5230\uff1a</p> \\[ 0 = \\tilde{\\mathbf{x}}^T_2 [\\mathbf{t}]_\\times \\tilde{\\mathbf{x}}_2 \\propto \\tilde{\\mathbf{x}}^T_2 [\\mathbf{t}]_\\times \\mathbf{R} \\tilde{\\mathbf{x}}_1 = \\tilde{\\mathbf{x}}^T_2 \\mathbf{E} \\tilde{\\mathbf{x}}_1 \\] <p>\u4e3a\u4ec0\u4e48 \\(0 = \\tilde{\\mathbf{x}}_2^T [\\mathbf{t}]_\\times \\tilde{\\mathbf{x}}_2\\) \u6210\u7acb ?</p> <ol> <li>\\([\\mathbf{t}]_\\times\\) \u662f\u53cd\u5bf9\u79f0\u77e9\u9635,\u6ee1\u8db3: \\([\\mathbf{t}]_\\times^T = -[\\mathbf{t}]_\\times\\)</li> <li>\u4efb\u610f\u5411\u91cf\\(\\mathbf{x}\\)\u4e0e\u53cd\u5bf9\u79f0\u77e9\u9635\\(\\mathbf{A}\\)\u7684\u4e58\u79ef\u6ee1\u8db3: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = 0\\) \u8bc1\u660e:</li> </ol> \\[\\begin{align*} \\mathbf{x}^T\\mathbf{A}\\mathbf{x} &amp;= (\\mathbf{x}^T\\mathbf{A}\\mathbf{x})^T &amp;&amp; (\\because \\mathbf{x}^T\\mathbf{A}\\mathbf{x}\u662f\u4e00\u4e2a\u6570)\\\\ &amp;= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{x}\\\\ &amp;= -\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\\\ &amp;= 0 \\end{align*} \\] <p>\u56e0\u6b64,\u6709: \\(0 = \\tilde{\\mathbf{x}}_2^T [\\mathbf{t}]_\\times \\tilde{\\mathbf{x}}_2\\)</p> <p>We arrive at the epipolar constraint(\u6781\u7ebf\u7ea6\u675f):</p> \\[ \\tilde{\\mathbf{x}}^T_2 \\tilde{\\mathbf{E}} \\tilde{\\mathbf{x}}_1 = 0 \\] <p>where \\(\\tilde{\\mathbf{E}} = [\\mathbf{t}]_\\times \\mathbf{R}\\) is the essential matrix.</p> <p>\\(\\tilde{\\mathbf{E}}\\) maps a point \\(\\tilde{\\mathbf{x}}_1\\) in image 1 to the corresponding epipolar line in image 2.</p> <p>while $$ \\tilde{\\mathbf{x}}_2^T \\tilde{\\mathbf{l}}_2 = 0 $$</p> <p>\u6211\u4eec\u6709\uff1a $$ \\tilde{\\mathbf{l}}_2 = \\tilde{\\mathbf{E}} \\tilde{\\mathbf{x}}_1 $$</p> <p>\u5bf9\u4e0a\u8ff0epipolar constraint\u53d6\u8f6c\u7f6e\uff0c\u4ee5\u53ca\\(\\tilde{\\mathbf{x}}_1^T \\tilde{\\mathbf{l}}_1 = 0\\)\uff0c\u8fd8\u53ef\u4ee5\u5f97\u5230\uff1a $$ \\tilde{\\mathbf{l}}_1 = \\tilde{\\mathbf{E}}^T \\tilde{\\mathbf{x}}_2 $$</p> <p>\u6211\u4eec\u5c06\u4e24\u4e2aepipolar\u4ee3\u5165\u4e0a\u9762epipolar constraint\u7684\u5f0f\u5b50\uff0c\u7531\u4e8e\u6211\u4eec\u53ea\u56fa\u5b9a\u4e00\u4e2aepipolar\uff0c\u53e6\u4e00\u70b9\u53ef\u4ee5\u4efb\u610f\u53d6\uff0c\u5f97\u5230\uff1a $$ \\tilde{\\mathbf{e}}_2^T \\tilde{\\mathbf{E}} = 0 ,\\tilde{\\mathbf{E}} \\tilde{\\mathbf{e}}_1 = 0 $$</p> <p>Thus, \\(\\tilde{\\mathbf{e}}_2^T\\) and \\(\\tilde{\\mathbf{e}}_1\\) are the left and right null-space vectors of \\(\\tilde{\\mathbf{E}}\\).</p>"},{"location":"CS/Note_03_Structure_from_Motion/#estimating-the-epipolar-geometry","title":"Estimating the Epipolar Geometry","text":"<p>We can recover the essential matrix \\(\\tilde{\\mathbf{E}}\\) from \\(N\\) image correspondences forming \\(N\\) homogeneous equations in the nine elements of \\(\\tilde{\\mathbf{E}}\\), using the epipolar constraint from above:</p> \\[ \\begin{align*} &amp;&amp;x_1x_2e_{11}&amp; &amp;+&amp; &amp;x_2y_1e_{12}&amp; &amp;+&amp; &amp;x_2e_{13}&amp;\\\\ &amp;+&amp; x_1y_2e_{11}&amp; &amp;+&amp; &amp;y_1y_2e_{12}&amp; &amp;+&amp; &amp;y_2e_{13}&amp;\\\\ &amp;+&amp; x_1e_{11}&amp; &amp;+&amp; &amp;y_1e_{12}&amp; &amp;+&amp; &amp;e_{13}&amp; = 0 \\end{align*} \\] <ul> <li>As \\(\\tilde{\\mathbf{E}}\\) is homogeneous, we use singular value decomposition to constrain the scale. This algorithm is also called the 8-point algorithm, because to solve this system of equations, we would need 8 points (because \\(\\tilde{\\mathbf{E}}\\) is a 3-by-3 matrix that is defined up to scale).</li> <li>However, the essential matrix actually has only 5 DoF (3 for rotation \\(\\mathbf{R}\\), 2 for translation \\(\\hat{\\mathbf{t}}\\)), and there exist algorithms that need fewer than 8 points.</li> <li>Note that some terms are products of two image measurements and hence amplify measurement noise asymmetrically. Thus, the normalized 8-point algorithm whitens the observations to have zero-mean and unit variance before the calculation and back-transforms the matrix recovered by SVD accordingly.</li> </ul> <p>From the essential matrix \\(\\tilde{\\mathbf{E}}\\), we can recover the direction \\(\\hat{\\mathbf{t}}\\) of the translation vector \\(\\mathbf{t}\\)</p> <p>Only the direction of \\(\\mathbf{t}\\) can be recovered, because the scale is not uniquely determined.</p> <p>\u6211\u4eec\u6709\uff1a$$ \\hat{\\mathbf{t}}^T \\tilde{\\mathbf{E}} = \\hat{\\mathbf{t}}^T [\\mathbf{t}]_\\times \\mathbf{R} = 0$$</p> <p>\u56e0\u6b64\uff0c\\(\\tilde{\\mathbf{E}}\\) is singular and we obtain \\(\\hat{\\mathbf{t}}\\) as the left singular vector associated with singular value 0. In practice the singular value will not be exactly 0 due to measurement noise, and we choose the smallest one. The other two singular values are roughly equal. The rotation matrix \\(\\mathbf{R}\\) can also be calculated,</p>"},{"location":"CS/Note_03_Structure_from_Motion/#estimating-the-epipolar-geometry-with-unknown-intrinsics","title":"Estimating the Epipolar Geometry with unknown Intrinsics","text":"<p>Up to now, we assumed the calibration matrices(\\(\\mathbf{K}_i\\)) to be known. However, in practice, we often do not know the intrinsics of the cameras. We cannot use the local ray directions \\(\\tilde{\\mathbf{x}}_i = \\mathbf{K}_i^{-1} \\bar{\\mathbf{x}}_i\\) anymore, but we can change the epipolar constraint to use the image coordinates \\(\\bar{\\mathbf{x}}_i\\) directly:</p> \\[ 0 = \\tilde{\\mathbf{x}}_2^T \\tilde{\\mathbf{E}} \\tilde{\\mathbf{x}}_1 = \\bar{\\mathbf{x}}_2^T \\mathbf{K}_2^{-T} \\tilde{\\mathbf{E}} \\mathbf{K}_1^{-1} \\bar{\\mathbf{x}}_1 = \\bar{\\mathbf{x}}_2^T \\tilde{\\mathbf{F}} \\bar{\\mathbf{x}}_1 \\] <p>where \\(\\tilde{\\mathbf{F}} = \\mathbf{K}_2^{-T} \\tilde{\\mathbf{E}} \\mathbf{K}_1^{-1}\\) is the fundamental matrix.</p> <p>Like \\(\\tilde{\\mathbf{E}}\\), \\(\\tilde{\\mathbf{F}}\\) is a rank-2 matrix and o and the epipoles can be recovered in the same way. However, the intrinsic parameters cannot be directly determined, i.e., we obtain only a perspective reconstruction and not a metric one. Additional information like vanishing points, a constancy of \\(\\mathbf{K}\\) across time, zero skew or an aspect ratio can be used to upgrade a perspective reconstruction to a metric one</p>"},{"location":"CS/Note_03_Structure_from_Motion/#323-triangulation","title":"3.2.3 Triangulation(\u4e09\u89d2\u5316)","text":"<p>Now that the camera intrinsics and extrinsics are known, how can we reconstruct the 3D points? In principle, this is easy: we just have to check where the two rays \\(\\tilde{\\mathbf{x}}_1\\) and \\(\\tilde{\\mathbf{x}}_2\\) intersect. However, due to measurement errors, the rays might not exactly intersect. So we want to find the point \\(\\mathbf{x}\\) that is closest to both rays </p> <p>Let \\(\\tilde{\\mathbf{x}}_i^s = \\tilde{\\mathbf{P}}_i\\tilde{\\mathbf{x}}_w\\) denote the projection of a 3D world point \\(\\tilde{\\mathbf{x}}_w\\) onto the image of the \\(i\\)th camera \\(\\tilde{\\mathbf{x}}_i^s\\). As both sides are homogeneous, they have the same direction but may differ in magnitude. To account for this, we consider the cross product \\(\\tilde{\\mathbf{x}}_i^s \\times \\tilde{\\mathbf{P}}_i \\tilde{\\mathbf{x}}_w = 0\\). Using \\(\\tilde{\\mathbf{p}}^T_{ik}\\) to denote the \\(k\\)th row of the \\(i\\)th camera\u2019s projection matrix \\(\\tilde{\\mathbf{P}}_i\\), we obtain:</p> \\[ \\begin{align*} &amp;\\tilde{\\mathbf{x}}_i^s \\times \\tilde{\\mathbf{P}}_i \\tilde{\\mathbf{x}}_w \\\\ =&amp; \\begin{pmatrix} x_i^s \\\\ y_i^s \\\\ 1 \\end{pmatrix} \\times \\begin{pmatrix} \\tilde{\\mathbf{p}}^T_{i1} \\\\ \\tilde{\\mathbf{p}}^T_{i2} \\\\ \\tilde{\\mathbf{p}}^T_{i3} \\end{pmatrix} \\tilde{\\mathbf{x}}_w \\\\ =&amp; \\begin{bmatrix} 0 &amp; -1 &amp; y_i^s \\\\ 1 &amp; 0 &amp; -x_i^s \\\\ -y_i^s &amp; x_i^s &amp; 0 \\end{bmatrix} \\begin{pmatrix} \\tilde{\\mathbf{p}}^T_{i1} \\\\ \\tilde{\\mathbf{p}}^T_{i2} \\\\ \\tilde{\\mathbf{p}}^T_{i3} \\end{pmatrix} \\tilde{\\mathbf{x}}_w \\\\ =&amp; \\begin{bmatrix} -\\tilde{\\mathbf{p}}^T_{i2} + y_i^s \\tilde{\\mathbf{p}}^T_{i3} \\\\ \\tilde{\\mathbf{p}}^T_{i1} - x_i^s \\tilde{\\mathbf{p}}^T_{i3} \\\\ x_i^s \\tilde{\\mathbf{p}}^T_{i2} - y_i^s \\tilde{\\mathbf{p}}^T_{i1} \\end{bmatrix} \\tilde{\\mathbf{x}}_w \\\\ \\end{align*} \\] <p>\u7531\u4e8e\u7b2c\u4e09\u884c\u662f\u7b2c\u4e00\u884c\u548c\u7b2c\u4e8c\u884c\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u6240\u4ee5\u7b2c\u4e09\u884c\u53ef\u4ee5\u4e0d\u7528\u8003\u8651\uff0c\u6211\u4eec\u6709\uff1a</p> \\[ \\begin{bmatrix} x_i^s \\tilde{\\mathbf{p}}^T_{i3} - \\tilde{\\mathbf{p}}^T_{i1} \\\\ y_i^s \\tilde{\\mathbf{p}}^T_{i3} - \\tilde{\\mathbf{p}}^T_{i2} \\end{bmatrix} \\tilde{\\mathbf{x}}_w = 0 \\] <p>Stacking \\(N \\geq 2\\) observations of a point, we obtain a linear system \\(\\mathbf{A}\\tilde{\\mathbf{x}}_w = 0\\). As \\(\\tilde{\\mathbf{x}}_w\\) is homogeneous this leads to a constrained least squares problem. The solution to this problem is the right singular vector corresponding to the smallest singular value of \\(\\mathbf{A}\\). This is the Direct Linear Transformation we are already familiar with from Lecture 2.</p> <p>Reprojection Error Minimization:</p> <p>While DLT often works well, it is not invariant to perspective transformations The gold standard is to minimize the reprojection error using numerical methods. This allows to take measurement noise appropriately into account:</p> \\[ \\bar{\\mathbf{x}}^*_w=\\arg\\min_{\\bar{\\mathbf{x}}_w} \\sum_{i=1}^N \\left\\| \\bar{\\mathbf{x}}^s_i(\\bar{\\mathbf{x}}_w)-\\bar{\\mathbf{x}}^o_i \\right\\|_2^2 \\] <p>where \\(\\bar{\\mathbf{x}}^s_i(\\bar{\\mathbf{x}}_w)\\) is the projection of \\(\\bar{\\mathbf{x}}_w\\) into the image of the \\(i\\)th camera and \\(\\bar{\\mathbf{x}}^o_i\\) is the observed image point.</p> <ul> <li>Triangulation works differently well depending on the relative camera pose.</li> <li>Note that the shaded region increases as the rays become more parallel (i.e. the camera poses become more similar).</li> <li>This means that there is a trade-off here: Feature matching becomes easier the closer the camera poses are, but triangulation becomes harder and vice-versa.</li> </ul> <p></p>"},{"location":"CS/Note_03_Structure_from_Motion/#33-factorization","title":"3.3 Factorization","text":"<p>\u89c6\u56fe\u589e\u52a0\uff01\uff1a Up to now, we have only talked about using two views to reconstruct 3D-geometry. Intuitively, we should get more accurate results if we use more than two views. We will talk about that approach in this section as well as the next.</p> <p>Let \\(\\mathcal{W} = {(x_{ip}, y_{ip})|i = 1, . . . , N, p = 1, . . . , P}\\) denote \\(P\\) feature points tracked over \\(N\\) frames. Given \\(\\mathcal{W}\\) and assuming orthographic projection our goal is to recover both camera motion (rotation) and the structure (3D points \\(\\tilde{\\mathbf{x_p}}\\) corresponding to \\((x_{ip}, y_{ip})\\)). We assume that all feature points are visible in all frames. The setup is visualized in the Figure below</p> <p></p> <p>Under orthographic projection, a 3D point \\(\\tilde{\\mathbf{x}}_p\\) maps to a pixel \\((x_{ip}, y_{ip})\\) in the \\(i\\)th frame as follows:</p> \\[ x_{ip}= \\mathbf{u}^T_i(\\mathbf{x}_p-\\mathbf{t}_i) \\\\  y_{ip}= \\mathbf{v}^T_i(\\mathbf{x}_p-\\mathbf{t}_i) \\] <p></p> <p>Without loss of generality, we assume that the 3D coordinate system is at the center: $$ \\frac{1}{P} \\sum_{p=1}^P \\mathbf{x}_p = \\mathbf{0} $$</p> <p>Let \\((x_{ip}, y_{ip})\\) denote the 2D location of feature \\(p\\) in frame \\(i\\). Centering the features per frame (zero-mean) and collecting them yields the centered measurement matrix \\(\\tilde{\\mathbf{W}}\\):</p> \\[ \\tilde{\\mathbf{W}} = \\begin{bmatrix} \\tilde{x}_{11} \\cdots \\tilde{x}_{1P} \\\\ \\vdots \\ddots \\vdots \\\\ \\tilde{x}_{N1} \\cdots \\tilde{x}_{NP} \\\\ \\tilde{y}_{11} \\cdots \\tilde{y}_{1P} \\\\ \\vdots \\ddots \\vdots \\\\ \\tilde{y}_{N1} \\cdots \\tilde{y}_{NP} \\end{bmatrix} \\] <p>where \\(\\tilde{x}_{ip} = x_{ip} - \\frac{1}{P} \\sum_{p=1}^P x_{iq}\\) and \\(\\tilde{y}_{ip} = y_{ip} - \\frac{1}{P} \\sum_{p=1}^P y_{iq}\\).</p> <ul> <li>zero-mean(\u96f6\u5747\u503c):\u8ba9\u6240\u6709\u8bad\u7ec3\u56fe\u50cf\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u50cf\u7d20\u5747\u503c\u4e3a0\uff0c\u4f7f\u5f97\u50cf\u7d20\u8303\u56f4\u53d8\u6210 [-128, 127]\uff0c\u4ee50\u4e3a\u4e2d\u5fc3\u3002\u662f\u6307\u53d8\u91cf\u51cf\u53bb\u5b83\u7684\u5747\u503c\uff1b</li> <li>\u8fd9\u91cc\u7684 ~ \u610f\u5473\u7740\u4e2d\u5fc3\u5316(centered)\uff0c\u800c\u4e0d\u662fhomogeneous\u3002</li> </ul> <p>Because</p> \\[ \\begin{align*}\\tilde{x}_{ip} &amp;= x_{ip} - \\frac{1}{P} \\sum_{p=1}^P x_{iq}\\\\ &amp;= \\mathbf{u}^T_i(\\mathbf{x}_p-\\mathbf{t}_i) - \\frac{1}{P} \\sum_{p=1}^P \\mathbf{u}^T_i(\\mathbf{x}_q-\\mathbf{t}_i)\\\\ &amp;= \\mathbf{u}^T_i(\\mathbf{x}_p-\\mathbf{t}_i) - \\mathbf{u}^T_i(\\frac{1}{P} \\sum_{p=1}^P \\mathbf{x}_q-\\mathbf{t}_i)\\\\ &amp;= \\mathbf{u}^T_i(\\mathbf{x}_p-\\mathbf{t}_i) - \\mathbf{u}^T_i(\\mathbf{0}-\\mathbf{t}_i)\\\\ &amp;= \\mathbf{u}^T_i\\mathbf{x}_p \\end{align*}\\] <p>For \\(\\tilde{y}_{ip}\\) we have: $$ \\tilde{y}_{ip} = \\mathbf{v}^T_i\\mathbf{x}_p$$</p> <p>The centered measurement matrix \\(\\tilde{\\mathbf{W}}\\) can be factorized into the product of a camera matrix \\(\\mathbf{R}\\)(represents the camera motion (rotation)) and a structure matrix \\(\\mathbf{X}\\):</p> \\[ \\tilde{\\mathbf{W} } = \\mathbf{R}\\mathbf{X} \\quad\\text{with} \\quad\\tilde{\\mathbf{W} }=\\begin{bmatrix} \\tilde{x}_{11} \\cdots \\tilde{x}_{1P} \\\\ \\vdots \\ddots \\vdots \\\\ \\tilde{x}_{N1} \\cdots \\tilde{x}_{NP} \\\\ \\tilde{y}_{11} \\cdots \\tilde{y}_{1P} \\\\ \\vdots \\ddots \\vdots \\\\ \\tilde{y}_{N1} \\cdots \\tilde{y}_{NP} \\end{bmatrix}, \\mathbf{R}=\\begin{bmatrix} \\mathbf{u}_1^T \\\\ \\vdots \\\\ \\mathbf{u}_N^T \\\\ \\mathbf{v}_1^T \\\\ \\vdots \\\\ \\mathbf{v}_N^T \\end{bmatrix}, \\mathbf{X}=\\begin{bmatrix} \\mathbf{x}_1 \\cdots \\mathbf{x}_P \\end{bmatrix} \\] <ul> <li>\\(\\mathbf{R}\\) is a \\(2N \\times 3\\) matrix and \\(\\mathbf{X}\\) is a \\(3 \\times P\\) matrix, so in the absence of noise, the centered measurement matrix \\(\\tilde{\\mathbf{W}}\\) has at most rank 3.</li> <li>When adding noise, the matrix becomes full rank(denota \\(\\hat{\\mathbf{W}}\\)).</li> </ul>"},{"location":"CS/Note_03_Structure_from_Motion/#331-svd","title":"3.3.1 SVD","text":"<ul> <li>We can recover the rank-3 matrix \\(\\tilde{\\mathbf{W}}\\) by doing a low-rank approximation of \\(\\hat{\\mathbf{W}}\\) using SVD (singular value decomposition):$$ \\hat{\\mathbf{W}} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$</li> <li>We obtain the rank 3 factorization by considering the singular vectors corresponding to the top 3 singular values (the others should be small and capture noise primarily):</li> </ul> \\[ \\hat{\\mathbf{R}}_{2N \\times 3} = \\mathbf{U}_{2N \\times 3}\\mathbf{\\Sigma}_{3 \\times 3}^{\\frac{1}{2}}, \\hat{\\mathbf{X}}_{3 \\times P} = \\mathbf{\\Sigma}_{3 \\times 3}^{\\frac{1}{2}}\\mathbf{V}_{3 \\times P}^T\\] <ul> <li>This approach minimizes the distance in Frobenius norm between the two matrices:\\(\\left\\| \\hat{\\mathbf{W}} - \\tilde{\\mathbf{W}} \\right\\|_F^2\\). while  making sure that \\(\\hat{\\mathbf{w}}\\) is of rank 3.</li> </ul> <p>Frobenius norm of a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) is defined as \\(\\|A\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}\\)</p>"},{"location":"CS/Note_03_Structure_from_Motion/#332-another-decomposition","title":"3.3.2 Another decomposition","text":"<ul> <li>However, this decomposition is not unique as there exists a matrix \\(\\mathbf{Q} \\in \\mathbb{R}^{3 \\times 3}\\) such that:$$ \\hat{\\mathbf{W}} = (\\hat{\\mathbf{R}}\\mathbf{Q})(\\mathbf{Q}^{-1}\\hat{\\mathbf{X}})$$</li> <li>We observe that the rows of \\(\\hat{\\mathbf{R}}\\) are unit vectors and the first half are orthogonal to the corresponding second half of \\(\\hat{\\mathbf{R}}\\)</li> </ul> <ul> <li>We can use this to constrain the factorization to be unique by requiring that the first half of the rows of \\(\\hat{\\mathbf{R}}\\) are orthogonal to the second half of the rows of \\(\\hat{\\mathbf{R}}\\).</li> <li>We obtain the metric constraint:</li> </ul> \\[\\begin{align*} \\hat{\\mathbf{u}}_i^T \\mathbf{Q} (\\hat{\\mathbf{u}}_i^T \\mathbf{Q})^T = 1,\\\\ \\hat{\\mathbf{v}}_i^T \\mathbf{Q} (\\hat{\\mathbf{v}}_i^T \\mathbf{Q})^T = 1,\\\\ \\hat{\\mathbf{u}}_i^T \\mathbf{Q} (\\hat{\\mathbf{v}}_i^T \\mathbf{Q})^T = 0 \\end{align*}\\] <p>\u8fd9\u4e9b\u5f0f\u5b50\u6765\u786e\u4fdd\u7ecf\u7531\\(\\mathbf{Q}\\)\u53d8\u5316\u540e\uff0c\u957f\u5ea6\u4e0d\u53d8\uff0c\u4e14\u4e24\u4e2a\u5411\u91cf\u6b63\u4ea4\u3002\u7a33\u5b9a\u6027\u597d\u3002 This gives us a large set of linear equations for the entries in the matrix \\(\\mathbf{Q} \\mathbf{Q}^T\\), from which the matrix \\(\\mathbf{Q}\\) can be recovered using standard Cholesky decomposition. Thus, the final algorithm for recovering both the camera motion as well as the 3D structure is the following:</p>"},{"location":"CS/Note_03_Structure_from_Motion/#333-algorithm","title":"3.3.3 Algorithm","text":"<ol> <li>Take measurement \\(\\hat{\\mathbf{W}}\\)</li> <li>Compute SVD \\(\\hat{\\mathbf{W}} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\\) and keep the top 3 singular values</li> <li>Define \\(\\hat{\\mathbf{R}}_{2N \\times 3} = \\mathbf{U}_{2N \\times 3}\\mathbf{\\Sigma}_{3 \\times 3}^{\\frac{1}{2}}, \\hat{\\mathbf{X}}_{3 \\times P} = \\mathbf{\\Sigma}_{3 \\times 3}^{\\frac{1}{2}}\\mathbf{V}_{3 \\times P}^T\\)</li> <li>Solve for \\(\\mathbf{Q}\\) using the metric constraint</li> <li>Compute \\(\\mathbf{R} = \\hat{\\mathbf{R}}\\mathbf{Q}\\) and \\(\\mathbf{X} = \\mathbf{Q}^{-1}\\hat{\\mathbf{X}}\\)</li> </ol>"},{"location":"CS/Note_03_Structure_from_Motion/#334-advantages-and-disadvantages","title":"3.3.4 advantages and disadvantages","text":"<p>Advantages:</p> <p>It is very fast because it has a closed-form solution (which is determined up to an arbitrary global rotation); there are no local minima.</p> <p>Disadvantages:</p> <p>Complete feature tracks are required, which means that it cannot handle occlusions of feature points.</p>"},{"location":"CS/Note_03_Structure_from_Motion/#34-bundle-adjustment","title":"3.4 Bundle Adjustment","text":""}]}