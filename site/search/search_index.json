{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/","title":"Mathematics for Deep Learning","text":""},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#broadcasting","title":"Broadcasting","text":""},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#hadamard-product","title":"Hadamard product","text":""},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#vector-and-matrix-norms","title":"Vector and Matrix Norms","text":"<p>The $L_p$ norm of a vector $x \\in \\mathbb{R}^n$ is defined as</p> <p> </p> <p>The $L_2$ norm is also called the Euclidean norm.</p> <p>$|x|1 = \\sum{i=1}^n |x_i|$ $|x|2 = \\sqrt{\\sum{i=1}^n x_i^2}$ $|x|\\infty = \\max{i=1,\\ldots,n} |x_i|$</p> <p>Frobenius norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as</p> <p> </p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#eigen-and-singular-value-decomposition","title":"Eigen and Singular Value Decomposition","text":""},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#eigenvalue-decompositionevd","title":"Eigenvalue Decomposition(EVD)","text":"<p>Only be applied to square matrices</p> <p> </p> <p>$V$ is a matrix whose columns are the eigenvectors of $A$. $\\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$.</p> <p>Each real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ can be decomposed into</p> <p> </p> <p>where $Q$ is an orthogonal matrix whose columns are the eigenvectors of $A$ and $\\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$.</p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#singular-value-decompositionsvd","title":"Singular Value Decomposition(SVD)","text":"<p>For non-square matrices we can use singular value decomposition</p> <p> </p> <p>$U$ and $V$ are orthogonal matrices. $D$ is a diagonal matrix whose diagonal entries are the singular values of $A$. $U$ and $V$ are the left and right singular vectors of $A$.</p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#relationship-between-evd-and-svd","title":"Relationship between EVD and SVD","text":"<p>The right singular vectors $V$ of $A$ are the eigenvectors of $A^T A$. The left singular vectors $U$ of $A$ are the eigenvectors of $A A^T$. The singular values of $A$ are the square roots of the eigenvalues of $A^T A$ or $A A^T$.</p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#application-principal-component-analysis-pca","title":"Application: Principal Component Analysis (PCA)","text":"<p>PCA is a technique for analyzing large high-dimensional datasets</p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#probability","title":"Probability","text":"<p>To do</p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#information-and-entropy","title":"Information and Entropy","text":"<p>To satisfy all requirements, we define the self-information of an event $X = x$ as</p> <p>  [unit:nat]</p> <p>Self-information is also interpreted as quantifying the level of \u201csurprise\u201d When using base-2 logarithms, units are called \u201cbits\u201d or \u201cshannons\u201d</p>"},{"location":"Note_00_Mathematics%20for%20Deep%20Learning/#the-argmin-and-argmax-operators","title":"The Argmin and Argmax Operators","text":"<p>Let $\\mathcal{X}$ be a set and $f: \\mathcal{X} \\rightarrow \\mathbb{R}$ be a function. The argmin of $f$ is defined as</p> <p> </p> <p>The argmax of $f$ is defined as</p> <p> </p>"},{"location":"Note_01_introduction/","title":"Computer Vision \u2013 Summer 2022","text":""},{"location":"Note_01_introduction/#organization","title":"Organization","text":"<p>Goal: Students gain an understanding of the theoretical and practical concepts of computer vision using deep neural networks and graphical models. A strong focus is put on 3D vision. After this course, students should be able to develop and train computer vision models, reproduce research results and conduct original research.</p> <p>Course website: https://uni-tuebingen.de/de/203146</p> <p>Exercises are offered every 2 weeks, 6 assignments in total</p>"},{"location":"Note_01_introduction/#course-materials","title":"Course Materials","text":"<p>Books:</p> <ul> <li>Szeliski: Computer Vision: Algorithms and Applications</li> <li>Hartley and Zisserman: Multiple View Geometry in Computer Vision</li> <li>Nowozin and Lampert: Structured Learning and Prediction in Computer Vision</li> <li>Goodfellow, Bengio, Courville: Deep Learning</li> <li>Deisenroth, Faisal, Ong: Mathematics for Machine Learning</li> <li>Inoffical lecture notes written by students in summer 2021</li> </ul> <p>Tutorials:</p> <ul> <li>The Python Tutorial</li> <li>NumPy Quickstart</li> <li>PyTorch Tutorial</li> </ul> <p>Frameworks / IDEs:</p> <ul> <li>Visual Studio Code</li> <li>Google Colab</li> </ul> <p>Related Courses:</p> <ul> <li>Gkioulekas (CMU): Computer Vision</li> <li>Owens (University of Michigan): Foundations of Computer Vision</li> <li>Lazebnik (UIUC): Computer Vision</li> <li>Freeman and Isola (MIT): Advances in Computer Vision</li> <li>Seitz (University of Washington): Computer Vision</li> <li>Slide Decks covering Szeliski Book</li> </ul>"},{"location":"Note_01_introduction/#prerequisites","title":"Prerequisites","text":"<p>Math:</p> <ul> <li>Linear algebra, probability and information theory. If unsure, have a look at: Goodfellow et al.: Deep Learning (Book), Chapters 1-4</li> <li>Luxburg: Mathematics for Machine Learning (Lecture)</li> <li>Deisenroth et al.:Mathematics for Machine Learning (Book)</li> </ul> <p>Computer Science:</p> <ul> <li>Variables, functions, loops, classes, algorithms</li> </ul> <p>Deep Learning:</p> <ul> <li>Geiger: Deep Learning (Lecture)</li> </ul> <p>Python and PyTorch:</p> <ul> <li>The Python Tutorial</li> <li>PyTorch Tutorial</li> </ul>"},{"location":"Note_01_introduction/#introduction","title":"Introduction","text":"<p>Goal of Computer Vision is to convert light into meaning (geometric, semantic, . . . )</p>"},{"location":"Note_01_introduction/#history","title":"History","text":"<p>Hard to say...</p>"},{"location":"Note_02_Image_Formation/","title":"Image_Formation","text":""},{"location":"Note_02_Image_Formation/#21-primitives-and-transformations","title":"2.1 Primitives and Transformations","text":"<p>Geometric primitives(\u51e0\u4f55\u56fe\u5143) are the basic building blocks used to describe 3D shapes, such as points, lines, planes, and polygons.</p>"},{"location":"Note_02_Image_Formation/#2d-primitives","title":"2D Primitives","text":""},{"location":"Note_02_Image_Formation/#2d-points","title":"2D Points","text":"<p>2D points can be written in inhomogeneous coordinates(\u975e\u9f50\u6b21\u5750\u6807) as</p> <p> </p> <p>or in homogeneous coordinates(\u9f50\u6b21\u5750\u6807) as</p> <p> </p> <p>where $ \\mathbb{P}^2 ( = \\mathbb{R}^3)$ is the projective plane(\u5c04\u5f71\u7a7a\u95f4) and $\\widetilde{\\mathbf{x}}$ is a projective point(\u6295\u5f71\u70b9).</p> <p>The tilde sign (~) is a convention for homogeneous coordinates. Homogeneous vectors are defined only up to scale.</p> <p>Inhomogeneous vector $\\mathbf{x}$ is converted to a homogeneous vector $\\widetilde{\\mathbf{x}}$ as follows:</p> <p> </p> <p>where $\\overline{x}$ is the augmented vector(\u589e\u5e7f\u5411\u91cf) of $\\mathbf{x}$.</p> <p>We say augmented vector $\\overline{\\mathbf{x} }$ for all homogeneous vectors which last coordinate is equal to 1. Special vector: Homogeneous points whose last element is $\\widetilde{w}=0$ are called ideal points or points at infinity. These points can\u2019t be represented with inhomogeneous coordinates!</p>"},{"location":"Note_02_Image_Formation/#2d-lines","title":"2D Lines","text":"<p>2D lines can also be expressed using homogeneous coordinates(\u9f50\u6b21\u5750\u6807) $\\mathbf{\\widetilde{l}}=(a,b,c)^T$ :  </p> <p>\u5de6\u5230\u53f3\u662f\u56e0\u4e3a $\\mathbf{\\overline{x}}^T\\mathbf{\\widetilde{l}}=0 \\Rightarrow (x,y,1)=0 \\Rightarrow ax+by+c=0$</p> <p>\u6807\u51c6\u5316 $\\mathbf{\\widetilde{l}}$\uff1a$\\mathbf{\\widetilde{l}}=(n_x,n_y,d)^T$\uff0c \u5176\u4e2d $n_x^2+n_y^2=1$\uff0c $(n_x,n_y)$ \u662f $\\mathbf{\\widetilde{l}}$ \u7684\u6cd5\u5411\u91cf \uff0c$d$ \u662f $\\mathbf{\\widetilde{l}}$ \u5230\u539f\u70b9\u7684\u8ddd\u79bb.</p> <p>An exception is the line at infinity $\\mathbf{\\widetilde{l}}_{\\infty}=(0,0,1)^T$, which passes through all ideal points.</p>"},{"location":"Note_02_Image_Formation/#2d-line-arithmetic","title":"2D Line Arithmetic","text":"<p>In homogeneous coordinates, the intersection of two lines is given by:  </p> <p>where $\\times$ is the cross product of two vectors.</p> <p>Similarly, the intersection of two points is given by:  </p>"},{"location":"Note_02_Image_Formation/#2d-conics","title":"2D Conics(\u4e8c\u6b21\u66f2\u7ebf)","text":"<p>More complex algebraic objects can be represented using polynomial homogeneous equations. For example, conic sections (arising as the intersection of a plane and a 3D cone) can be written using quadric equations:</p> <p> </p> <p></p>"},{"location":"Note_02_Image_Formation/#3d-primitives","title":"3D Primitives","text":""},{"location":"Note_02_Image_Formation/#3d-points","title":"3D Points","text":"<p>3D points can be written in inhomogeneous coordinates as  or in homogeneous coordinates as  </p> <p>where $\\mathbb{P}^3(=\\mathbb{R}^4)$ is the projective space.</p>"},{"location":"Note_02_Image_Formation/#3d-planes","title":"3D Planes","text":"<p>3D planes can be expressed using homogeneous coordinates $\\mathbf{\\widetilde{m}}=(a,b,c,d)^T$:  </p> <p>\u6807\u51c6\u5316 $\\mathbf{\\widetilde{m}}$\uff1a$\\mathbf{\\widetilde{m}}=(n_x,n_y,n_z,d)^T$ \u5176\u4e2d $n_x^2+n_y^2+n_z^2=1$, $(n_x,n_y,n_z)$ \u662f $\\mathbf{\\widetilde{m}}$ \u7684\u6cd5\u5411\u91cf\uff0c$d$ \u662f $\\mathbf{\\widetilde{m}}$ \u5230\u539f\u70b9\u7684\u8ddd\u79bb\u3002</p> <p>An exception is the plane at infinity $\\mathbf{\\widetilde{m}}_{\\infty}=(0,0,0,1)^T$, which passes through all ideal points.</p>"},{"location":"Note_02_Image_Formation/#3d-lines","title":"3D Lines","text":"<p>3D lines are less elegant than either 2D lines or 3D planes. One possible representation is to express points on a line as a linear combination of two points $\\mathbf{p}$ and $\\mathbf{q}$ on the line:</p> <p> </p> <p>However, this representation uses 6 parameters($\\mathbf{p}$\u548c$\\mathbf{q}$\u5206\u522b\u4e09\u4e2a\u53c2\u6570) for 4 degrees of freedom(\u4e00\u4e2a\u70b9(3\u4e2a\u4e0d\u81ea\u7531\u5ea6)+\u65b9\u5411(1\u4e2a\u4e0d\u81ea\u7531\u5ea6)).</p> <p>\u6240\u4ee5\u53ef\u4ee5\u7528two-plane parameterization\uff08\u4e24\u5e73\u9762\u53c2\u6570\u5316\uff09\u6765\u8868\u793a3D line.</p>"},{"location":"Note_02_Image_Formation/#3d-quadrics","title":"3D Quadrics(\u4e8c\u6b21\u66f2\u9762)","text":"<p>The 3D analog of 2D conics is a quadric surface:</p> <p> </p> <p></p> <p>Superquadrics: generalization of quadrics</p>"},{"location":"Note_02_Image_Formation/#2d-transformations","title":"2D Transformations","text":""},{"location":"Note_02_Image_Formation/#translation","title":"Translation","text":"<p>Translation: (2D Translation of the Input, 2 DoF(\u4e24\u4e2a\u81ea\u7531\u5ea6: $t_x$\u548c$t_y$))</p> <p> </p> <ul> <li>Using homogeneous representations allows to chain/invert transformations.</li> <li>Augmented vectors $\\mathbf{\\overline{x}}$ can always be replaced by general homogeneous ones $\\widetilde{\\mathbf{x}}$.</li> </ul>"},{"location":"Note_02_Image_Formation/#euclidean","title":"Euclidean","text":"<p>Euclidean: (2D Translation + 2D Rotation, 3 DoF)</p> <p> </p> <ul> <li>$\\mathbf{R} \\in SO(2)$: special orthogonal group of 2D rotations,which is an orthogonal rotation matrix with $\\mathbf{R}^T\\mathbf{R}=\\mathbf{I}$ and $\\det(\\mathbf{R})=1$.</li> <li>Euclidean transformations preserve Euclidean distances</li> </ul>"},{"location":"Note_02_Image_Formation/#similarity","title":"Similarity","text":"<p>Similarity: (2D Translation + Scaled 2D Rotation, 4 DoF)</p> <p> </p> <ul> <li>$s$ is the scale factor.</li> <li>The similarity transform preserves angles between lines</li> </ul>"},{"location":"Note_02_Image_Formation/#affine","title":"Affine","text":"<p>Affine: (2D Translation + 2D Rotation + 2D Shear, 6 DoF)</p> <p> </p> <ul> <li>$\\mathbf{A}$ is an arbitrary 2D matrix.</li> <li>Parallel lines remain parallel under affine transformations</li> </ul>"},{"location":"Note_02_Image_Formation/#projective","title":"Projective","text":"<p>Projective: (2D Translation + 2D Rotation + 2D Shear + 2D Homography, 8 DoF)\uff08\u5bf9\u540c\u6784\u77e9\u9635\u6574\u4f53\u4e58\u4ee5\u4e00\u4e2a\u5e38\u6570\u5e76\u4e0d\u6539\u53d8\u5c04\u5f71\u53d8\u6362,\u6240\u4ee5\u53ef\u4ee5\u8ba9\u5176\u4e2d\u4e00\u4e2a\u5143\u7d20\u53d6\u4efb\u610f\u5b9a\u503c(\u4f8b\u5982\u53d61),\u7b49\u4ef7\u53bb\u6389\u4e00\u4e2a\u81ea\u7531\u5ea6\uff09</p> <p> </p> <ul> <li>$\\mathbf{\\widetilde{H}}$ is an arbitrary homogeneous 3x3 matrix.</li> <li>Projective transformations preserve straight lines</li> </ul>"},{"location":"Note_02_Image_Formation/#2d-transformations-on-co-vectors","title":"2D Transformations on Co-vectors","text":"<p>Consider a covector $\\mathbf{\\widetilde{l}}=(a,b,c)^T$ representing a 2D line. How does it transform under a 2D transformation $\\mathbf{H}$?</p> <p>Answer: $\\mathbf{\\widetilde{l}'}=\\mathbf{H}^{-T}\\mathbf{\\widetilde{l}}$</p> <p>Solution: Consider a point $\\mathbf{\\overline{x}}$ on the line $\\mathbf{\\widetilde{l}}$:</p> <p>Considering any perspective 2D transformation:  </p> <p>the transformed 2D line equation is given by:  </p> <p>So:  </p> <p>which implies:  </p> <p>Thus, the action of a projective transformation on a co-vector such as a 2D line or 3D normal can be represented by the transposed inverse of the matrix.</p>"},{"location":"Note_02_Image_Formation/#overview-of-2d-transformations","title":"Overview of 2D Transformations","text":"Transformation DoF Preserves Matrix Translation 2 Orientation $_{2\\times 3}$ Rigid(Euclidean) 3 Length $_{2\\times 3}$ Similarity 4 Angles $_{2\\times 3}$ Affine 6 Parallelism $_{2\\times 3}$ Projective 8 Straight lines $_{3\\times 3}$ <ul> <li>Interpret as restricted $3 \u00d7 3$ matrices operating on 2D homogeneous coordinates</li> <li>Transformations preserve properties below</li> </ul>"},{"location":"Note_02_Image_Formation/#overview-of-3d-transformations","title":"Overview of 3D Transformations","text":"Transformation DoF Preserves Matrix Translation 3 Orientation $_{3\\times 4}$ Rigid(Euclidean) 6 Length $_{3\\times 4}$ Similarity 7 Angles $_{3\\times 4}$ Affine 12 Parallelism $_{3\\times 4}$ Projective 15 Straight lines $_{4\\times 4}$ <ul> <li>3D transformations are defined analogously(\u7c7b\u4f3c\u7684) to 2D transformations</li> <li>$3 \u00d7 4$ matrices are extended with a fourth $$ row for homogeneous transforms.</li> <li>Transformations preserve properties below (similarity: parallelism, straight lines)</li> </ul>"},{"location":"Note_02_Image_Formation/#direct-linear-transform-for-homography-estimation","title":"Direct Linear Transform for Homography Estimation","text":"<p>\u7528\u76f4\u63a5\u7ebf\u6027\u53d8\u6362(DLT)\u4f30\u8ba1\u5355\u5e94\u6027\u77e9\u9635(Homography Matrix)</p> <p>How can we estimate a homography from a set of 2D correspondences?</p> <ul> <li>2D correspondences: $\\mathbf{x}_i \\leftrightarrow \\mathbf{x}'_i$</li> <li>$\\mathbf{x}_i$ is a point in the first image</li> <li>$\\mathbf{x}'_i$ is the corresponding point in the second image</li> <li>Homography: $\\mathbf{\\widetilde{x}}'_i=\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i$</li> </ul> <p>Let $\\mathcal{X}={\\mathbf{\\widetilde{x}i,\\widetilde{x}'_i}}{i=1}^N$denote a set of $N$ 2D-to-2D correspondences related by $\\mathbf{\\widetilde{x}}'_i=\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i$.As the correspondence vectors are homogeneous, they have the same direction but differ in magnitude.</p> <p>Why to say \"the same direction but differ in magnitude\"? See the following picture: </p> <p>Thus, the equation above can be expressed as $\\mathbf{\\widetilde{x}}'_i\\times\\mathbf{\\widetilde{H}}\\mathbf{\\widetilde{x}}_i=0$. Using the $\\mathbf{\\widetilde{h}}^T_k$ to denote the k-th row of $\\mathbf{\\widetilde{H}}$, we can write:</p> <p>Note:</p> <ol> <li>$\\mathbf{a}\\times\\mathbf{b}= =$</li> </ol> <p>\u5c55\u5f00\u53c9\u4e58\u8fd0\u7b97: $ $</p> <ul> <li>\u7b2c\u4e8c\u6b65\u5230\u7b2c\u4e09\u6b65\u662f\u56e0\u4e3a $\\mathbf{h}_i^T \\mathbf{\\widetilde{x}}_i$\u662f\u4e00\u4e2a\u6570\uff0c\u6240\u4ee5$\\mathbf{h}_i^T \\mathbf{\\widetilde{x} }_i = \\mathbf{\\widetilde{x} }_i^T \\mathbf{h}_i$\u3002</li> <li>\u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u5de6\u8fb9\u7684\u6570\u636e\u6765\u4f30\u8ba1 $\\mathbf{H}$ \u4e86\u3002</li> <li>\u6700\u540e\u4e00\u884c\u7684\u4e09\u4e2a\u65b9\u7a0b\u662f\u5197\u4f59\u7684\uff0c\u56e0\u4e3a$-\\widetilde{w}'_iR_3 = \\widetilde{x}'_i R_1 + \\widetilde{y}'_i R_2$\u3002</li> </ul> <p>Each point correspondence yields two equations. Stacking all equations into a $2N \u00d7 9$(\u4e00\u7ec4\u6709\u4e24\u4e2a\u65b9\u7a0b\uff0c\u5217\u6570\u5747\u4e3a9) dimensional matrix $\\mathbf{A}$ leads to the following constrained least squares problem</p> <p>\u7531\u4e8e\u8bef\u5dee\u7684\u5b58\u5728\uff0c\u4e0a\u5f0f\u53ef\u80fd\u4e0d\u7b49\u4e8e0\uff0c\u6240\u4ee5\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6765\u4f30\u8ba1 $\\mathbf{H}$:</p> <p>\u4ee4 $\\mathbf{A} \\widetilde{\\mathbf{h}} = \\omega$</p> <p>$\\Omega = \\omega^T \\omega $</p> <p>\u6c42\u89e3 $\\widetilde{\\mathbf{h}}$ \u4f7f\u5f97 $\\Omega$ \u6700\u5c0f\u4e5f\u5c31\u662f</p> <p> </p> <p>\u5176\u4e2d $\\widetilde{\\mathbf{h}}^*$ \u662f $\\mathbf{H}$ \u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\u3002</p> <p>where $\\lambda$ is a Lagrange multiplier(\u62c9\u683c\u6717\u65e5\u4e58\u5b50) to enforce the constraint $|\\widetilde{\\mathbf{h}}|^2=1$.</p> <p>The solution to the above optimization problem is the singular vector corresponding to the smallest singular value of $\\mathbf{A}$.(i.e., the last column of $V$ when decomposing $A = UDV^T$ , see also Deep Learning lecture 11.2). The resulting algorithm is called Direct Linear Transformation.</p>"},{"location":"Note_02_Image_Formation/#22-geometric-image-formation","title":"2.2 Geometric Image Formation","text":""},{"location":"Note_02_Image_Formation/#basic-camera-models","title":"Basic camera models","text":"<p>Physical Camera Model </p> <p>Mathematical Camera Model </p>"},{"location":"Note_02_Image_Formation/#projection-models","title":"Projection models","text":""},{"location":"Note_02_Image_Formation/#orthographic-projection","title":"Orthographic Projection*","text":"<p>An orthographic projection simply drops the z component of the 3D point in camera coordinates $\\mathbf{x}_c$ to obtain the corresponding 2D point on the image plane (= screen) $\\mathbf{x}_s$.</p> <p></p> <p> </p> <ul> <li>remember that $\\overline{\\mathbf{x}}_c$ is the augmented vector of $\\mathbf{x}_c$.</li> <li>Orthography is exact for telecentric lenses and an approximation for telephoto lenses. After projection the distance of the 3D point from the image can\u2019t be recovered</li> </ul>"},{"location":"Note_02_Image_Formation/#scaled-orthographic-projection","title":"Scaled Orthographic Projection","text":"<p>In practice, world coordinates (which may measure dimensions in meters) must be scaled to fit onto an image sensor (measuring in pixels) \u21d2 scaled orthography:</p> <p> </p> <p>The unit for s is px/m or px/mm to convert metric 3D points into pixels</p> <p>Under orthography, structure and motion can be estimated simultaneously using factorization methods (e.g., via singular value decomposition).</p>"},{"location":"Note_02_Image_Formation/#perspective-projection","title":"Perspective Projection**","text":"<p>For perspective projection, we can use the following relationship (which is just from the principle of equal triangles):  </p> <p></p> <p>In perspective projection, 3D points in camera coordinates are mapped to the image plane by dividing them by their z component and multiplying with the focal length:</p> <p> </p> <ul> <li>Note that this projection is linear when using homogeneous coordinates. After the projection it is not possible to recover the distance of the 3D point from the image.</li> <li>Remark: The unit for $f$ is px (=pixels) to convert metric 3D points into pixels.</li> </ul>"},{"location":"Note_02_Image_Formation/#principal-point-offset","title":"Principal Point Offset","text":"<p>Usually in practice, we compute the principal point offset to derive a coordinate system for the image plane that is much more convenient as it does not include negative values.</p> <ol> <li> <p>Without principal point offset </p> </li> <li> <p>With principal point offset </p> </li> </ol> <p>This moves the image coordinate system to the corner of the image plane</p>"},{"location":"Note_02_Image_Formation/#complete-perspective-projection-model","title":"Complete perspective projection model","text":"<p>The complete perspective projection model is given by:</p> <p> </p> <ul> <li>The Matrix $\\mathbf{K}=$ is called the calibration matrix.</li> <li>The parameters of K are called camera intrinsics (as opposed to extrinsic pose)</li> <li>Here, $f_x$ and $f_y$ are independent, allowing for different pixel aspect ratios</li> <li>The skew parameter $s$ arises due to the sensor not mounted perpendicular to the optical axis</li> <li>In practice, we often set $s=0$ and $f_x=f_y=f$ to simplify the model</li> </ul>"},{"location":"Note_02_Image_Formation/#chaining-transformations","title":"Chaining Transformations","text":"<p>Let $\\mathbf{K}$ be the calibration matrix (intrinsics) and $$ be the camera pose (extrinsics). We chain both transformations to project a point in world coordinates to the image:</p> <p> </p> <p></p> <p>The matrix $\\mathbf{P}=\\mathbf{K}$ is called the camera matrix, which is a $3 \u00d7 4$ matrix, can be precomputed and is used to project 3D points to the image plane.</p>"},{"location":"Note_02_Image_Formation/#full-rank-representation","title":"Full Rank Representation","text":"<p>It is sometimes preferable to use a full rank 4 \u00d7 4 projection matrix:</p> <p> </p> <p>Now, the homogeneous vector $\\widetilde{\\mathbf{x}}_s$ is a 4D vector and must be normalized wrt. its 3rd entry to obtain inhomogeneous image pixels:  Note that the 4th component of the inhomogeneous 4D vector is the inverse depth. If the inverse depth is known, a 3D point can be retrieved from its pixel coordinates via $\\widetilde{\\mathbf{x}}_w=\\mathbf{P}^{-1}\\overline{\\mathbf{x}}_s$ and subsequent normalization of $\\widetilde{\\mathbf{x}}$ wrt. its 4th entry.</p>"},{"location":"Note_02_Image_Formation/#lens-distortion","title":"Lens Distortion","text":"<p>The assumption of linear projection (straight lines remain straight) is violated in practice due to the properties of the camera lens which introduces distortions. Both radial and tangential distortion effects can be modeled relatively easily: Let $x = x_c/z_c, y = y_c/z_c$ and $r^2 = x^2 + y^2$ . The distorted point is obtained as:</p> <p> </p> <p>where $\\kappa_1, \\kappa_2$ are the radial distortion coefficients and $\\tau_1, \\tau_2$ are the tangential distortion coefficients.</p> <p>Images can be undistorted such that the perspective projection model applies. Note here, that is practice, this is already done before the image is computed. More complex distortion models must be used for wide-angle lenses</p>"},{"location":"Note_02_Image_Formation/#23-photometric-image-formation","title":"2.3 Photometric Image Formation","text":"<p>We now discuss how an image is formed in terms of pixel intensities and colors.</p>"},{"location":"Note_02_Image_Formation/#rendering-equation","title":"Rendering Equation","text":"<p>Let $\\mathbf{p} \\in \\mathbb{R}^3$ denote  a 3D surface point, $\\mathbf{v} \\in \\mathbb{R}^3$ the viewing direction and $\\mathbf{s} \\in \\mathbb{R}^3$ the incoming light direction. The rendering equation describes how much of the light $L_{in}$ with wavelength $\\lambda$ arriving at $\\mathbf{p}$ is reflected into the viewing direction $\\mathbf{v}$:</p> <p> </p> <ul> <li>$\\Omega$ is the unit hemisphere at normal $\\mathbf{n}$.</li> <li>$BRDF$ is the bidirectional reflectance distribution function defines how light is reflected at an opaque surface(\u4e0d\u900f\u660e\u8868\u9762), \u8868\u793a\u4ece\u65b9\u5411$\\mathbf{s}$\u5165\u5c04\u7684\u5149\u6709\u591a\u5c11\u6bd4\u4f8b\u4f1a\u53cd\u5c04\u5230\u65b9\u5411$\\mathbf{v}$</li> <li>$L_{emit} &gt; 0$ only for light emitting surfaces, \u63cf\u8ff0\u7684\u662f\u70b9p\u4f5c\u4e3a\u5149\u6e90\u81ea\u8eab\u53d1\u51fa\u7684\u5149,\u4e0d\u4f9d\u8d56\u4e8e\u5916\u754c\u7684\u5165\u5c04\u5149\u3002\u6bd4\u5982\u4e00\u4e2a\u53d1\u5149\u7684\u7403\u4f53\u8868\u9762,\u5b83\u81ea\u8eab\u5c31\u4f1a\u53d1\u51fa\u5149\u7ebf\u3002</li> <li>$(-\\mathbf{n}^T\\mathbf{s})$ attenuation equation (if light arrives exactly perpendicular, there is no reflectance at all. or if it arrives at a shallow angle, there is less light reflected).</li> <li>\u6839\u636eLambert cosine law,\u4e00\u4e2a\u8868\u9762\u7684\u53cd\u5c04\u4eae\u5ea6\u4e0e\u5149\u6e90\u5165\u5c04\u89d2\u7684\u4f59\u5f26\u503c\u6210\u6b63\u6bd4\u3002</li> <li>\u5176\u5b9e\u5c31\u662f$\\mathbf{n}$\u548c$\\mathbf{s}$\u7684\u5939\u89d2\u7684\u4f59\u5f26\u503c\u3002</li> <li>\u4f59\u5f26\u503c\u6b63\u53f7\u8868\u793a\u5149\u7167\u65b9\u5411\u548c\u6cd5\u7ebf\u540c\u5411,\u8d1f\u53f7\u8868\u793a\u5149\u7167\u65b9\u5411\u548c\u6cd5\u7ebf\u53cd\u5411\u3002\u52a0\u5165\u8d1f\u53f7\u662f\u4e3a\u4e86\u8ba1\u7b97\u5149\u7167\u4ece\u80cc\u9762\u5165\u5c04\u65f6\u7684\u6b63\u786e\u6548\u679c</li> </ul>"},{"location":"Note_02_Image_Formation/#brdf-components","title":"BRDF Components","text":"<p>Typical BRDFs have a diffuse(\u6f2b\u53cd\u5c04) and a specular(\u955c\u9762\u53cd\u5c04) component:</p> <p></p> <ul> <li>\u56e0\u4e3a\u8868\u9762\u4e0d\u4e00\u5b9a\u662f\u5b8c\u5168\u5149\u6ed1\u7684,\u6240\u4ee5\u4f1a\u5448\u73b0Specular\u7684\u6548\u679c</li> <li>The specular component depends strongly on the outgoing light direction.\uff08\u60f3\u60f3\u5851\u6599\u7684\u5404\u4e2a\u89d2\u5ea6\u7684\u6548\u679c\uff09</li> </ul>"},{"location":"Note_02_Image_Formation/#brdf-in-ractice","title":"BRDF in ractice","text":""},{"location":"Note_02_Image_Formation/#fresnel-effect","title":"Fresnel Effect","text":"<p>The specular component can get stronger if the surface is further away because the viewing angle changes (example with the water reflectance). So the amount of relectance depends on the viewing angle.</p>"},{"location":"Note_02_Image_Formation/#global-illumination","title":"Global Illumination","text":"<p>Modeling one light bounce is insufficient for rendering complex scenes. Light sources can be shadowed by occluders and rays can bounce multiple times. Global illumination techniques also take indirect illumination into account.</p>"},{"location":"Note_02_Image_Formation/#camera-lenses","title":"Camera lenses","text":""},{"location":"Note_02_Image_Formation/#the-thin-lens-model","title":"The thin lens model","text":"<ul> <li>$f$ is the focal length</li> <li>$z_s$ is the distance from the lens to the image plane</li> <li>$z_c$ is the distance from the lens to the object plane</li> </ul> <p>The thin lens model with spherical lens is often used as an approximation. Properties: Axis-parallel rays pass the focal point, rays via center keep direction. From Snell\u2019s law we obtain $f=\\frac{R}{2(n-1)}$ with radius $R$ and index of refraction $n$.</p>"},{"location":"Note_02_Image_Formation/#depth-of-field","title":"Depth of Field(\u666f\u6df1)","text":"<p>\u8fd9\u91cc\u7684$\\Delta z_s$\u662f\u6307\u5728\u7126\u5e73\u9762\u4e0a\u7684\u666f\u6df1\u8303\u56f4\uff0c\u662f\u524d\u666f\u548c\u540e\u666f\u7684\u8ddd\u79bb\u3002</p> <p>If the image plane is out of focus, a 3D point projects to the circle of confusion $c$. The circle of confusion is a little disc - if the disc is larger than the pixel size, we get blur!</p> <p>The allowable depth variation that limits the circle of confusion $c$ is called depth of field and is a function of both the focus distance and the lens aperture. Typical DSLR cameras have a Depth of Field Indicator which describes the range where the image appears sharply. The commonly displayed f-number is defined as  where $f$ is the focal length and $d$ is the diameter of aperture\uff08\u5149\u5708\u76f4\u5f84\uff09. The f-number is inversely proportional to the aperture diameter.</p> <p>To control the size of the circle of confusion, we change the lens aperture. An aperture is a hole or an opening through which light travels. The aperture limits the amount of light that can reach the image plane. Smaller apertures lead to sharper, but more noisy images (less photons)</p>"},{"location":"Note_02_Image_Formation/#chromatic-aberration","title":"Chromatic Aberration(\u8272\u5dee)","text":""},{"location":"Note_02_Image_Formation/#vignetting","title":"Vignetting(\u6655\u5f71)","text":""},{"location":"Note_02_Image_Formation/#image-sensing-pipeline","title":"Image Sensing Pipeline","text":"<p>The image sensing pipeline can be divided into three stages:</p> <ul> <li>Physical light transport in the camera lens/body</li> <li>Optics(\u5149\u5b66) + Aperture(\u5149\u5708) + Shutter(\u5feb\u95e8)</li> <li>Photon measurement and conversion on the sensor chip(\u4f20\u611f\u5668\u82af\u7247)</li> <li>Sensor(\u4f20\u611f\u5668) + Gain(ISO)(\u589e\u76ca) + ADC(\u6a21\u6570\u8f6c\u6362\u5668)</li> <li>Image signal processing (ISP) and image compression</li> <li>Demosaic(\u53bb\u9a6c\u8d5b\u514b) + Denoise and sharpen(\u53bb\u566a\u548c\u9510\u5316) + White balance(\u767d\u5e73\u8861) + Gamma/curve(\u4f3d\u9a6c/\u66f2\u7ebf) + Compress(\u538b\u7f29)</li> </ul>"},{"location":"Note_02_Image_Formation/#shutter","title":"Shutter","text":"<p>A focal plane shutter(\u7126\u5e73\u9762\u5feb\u95e8) is positioned just in front the image sensor / film. Most digital cameras use a combination of mechanical and electronic shutter. The shutter speed (exposure time) controls how much light reaches the sensor It determines if an image appears over- (too light)/underexposed (too dark), blurred (motion blur) or noisy.</p>"},{"location":"Note_02_Image_Formation/#sensor","title":"Sensor","text":"<p>Two main principles: CCD and CMOS for light sensors. CCDs move charge from pixel to pixel and convert it to voltage at the output node. CMOS images convert charge to voltage inside each pixel and are standard today</p>"},{"location":"Note_02_Image_Formation/#color-filter-arrays","title":"Color Filter Arrays","text":"<p>To measure color, pixels are arranged in a color array, e.g.: Bayer RGB pattern. Missing colors at each pixel are interpolated from the neighbors (demosaicing)</p> <p>Each pixel integrates the light spectrum $L$ according to its spectral sensitivity $S$:</p> <p> </p>"},{"location":"Note_02_Image_Formation/#different-color-spaces","title":"Different color spaces","text":"<p>RGB: red, green, blue Lab*: lightness, red-green, blue-yellow HSV: hue(\u8272\u8c03), saturation(\u9971\u548c\u5ea6), value(\u660e\u5ea6)</p>"},{"location":"Note_02_Image_Formation/#gamma-compression","title":"Gamma Compression(\u4f3d\u9a6c\u538b\u7f29)","text":"<ul> <li>Humans are more sensitive to intensity differences in darker regions</li> <li>Therefore, it is beneficial to nonlinearly transform (left) the intensities or colors prior to discretization (left) and to undo this transformation during loading</li> </ul>"},{"location":"Note_02_Image_Formation/#image-compression","title":"Image Compression","text":"<ul> <li>Often images are compressed into a format similar to JPEG.</li> <li>Typically luminance is compressed with higher fidelity than chrominance.</li> <li>Often, (8 \u00d7 8 pixel) patch-based discrete cosine or wavelet transforms are used.</li> <li>-Discrete Cosine Transform (DCT) is an approximation to PCA on natural images.</li> <li>The coefficients are quantized to integers that can be stored with Huffman codes.</li> <li>More recently, deep network based compression algorithms are developed (improving the compression a lot compared to DCT)</li> </ul>"}]}